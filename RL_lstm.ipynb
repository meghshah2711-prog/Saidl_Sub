{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euib_RmfOiT3"
      },
      "source": [
        "## install mujoco-py and D4RL\n",
        "\n",
        "* **Restart Runtime** after running this block to complete D4RL setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAGCHznQs2bI",
        "outputId": "dbe1b209-26a3-48c1-8e01-722dd8d2aed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 108 kB in 2s (55.4 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "E: Unable to find a source package for mesa\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "llvm-dev is already the newest version (1:10.0-50~exp1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3 is already the newest version (2.8.1-3).\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.8ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "net-tools is already the newest version (1.60+git20180626.aebd88e-1ubuntu1).\n",
            "libglew-dev is already the newest version (2.1.0-4).\n",
            "libglfw3-dev is already the newest version (3.3.2-1).\n",
            "patchelf is already the newest version (0.10-2build1).\n",
            "xpra is already the newest version (3.0.6+dfsg1-1build1).\n",
            "curl is already the newest version (7.68.0-1ubuntu2.18).\n",
            "git is already the newest version (1:2.25.1-1ubuntu3.11).\n",
            "libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
            "libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
            "libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
            "software-properties-common is already the newest version (0.99.9.11).\n",
            "unzip is already the newest version (6.0-25ubuntu1.1).\n",
            "vim is already the newest version (2:8.1.2269-1ubuntu5.15).\n",
            "wget is already the newest version (1.20.3-1ubuntu2).\n",
            "xserver-xorg-dev is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "virtualenv is already the newest version (20.0.17-1ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###### libs for install ######\n",
        "\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install gcc\n",
        "\n",
        "!sudo apt-get build-dep mesa\n",
        "!sudo apt-get install llvm-dev\n",
        "!sudo apt-get install freeglut3 freeglut3-dev\n",
        "\n",
        "!sudo apt-get install python3-dev\n",
        "\n",
        "!sudo apt-get install build-essential\n",
        "\n",
        "!sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n",
        "        libosmesa6-dev software-properties-common net-tools unzip vim \\\n",
        "        virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf\n",
        "\n",
        "#!sudo apt-get install -y libglew-dev\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23eqLoV_orip",
        "outputId": "9bfd6293-ba72-4e3d-a14e-7af2f7cededc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-05 11:49:22--  https://roboti.us/download/mujoco200_linux.zip\n",
            "Resolving roboti.us (roboti.us)... 104.40.85.93\n",
            "Connecting to roboti.us (roboti.us)|104.40.85.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4427362 (4.2M) [application/zip]\n",
            "Saving to: ‘mujoco200_linux.zip’\n",
            "\n",
            "mujoco200_linux.zip 100%[===================>]   4.22M  1.67MB/s    in 2.5s    \n",
            "\n",
            "2023-07-05 11:49:26 (1.67 MB/s) - ‘mujoco200_linux.zip’ saved [4427362/4427362]\n",
            "\n",
            "--2023-07-05 11:49:26--  https://roboti.us/file/mjkey.txt\n",
            "Resolving roboti.us (roboti.us)... 104.40.85.93\n",
            "Connecting to roboti.us (roboti.us)|104.40.85.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 768 [text/plain]\n",
            "Saving to: ‘mjkey.txt.1’\n",
            "\n",
            "mjkey.txt.1         100%[===================>]     768  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-05 11:49:26 (59.7 MB/s) - ‘mjkey.txt.1’ saved [768/768]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###### mujoco setup ######\n",
        "\n",
        "\n",
        "#!wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\n",
        "\n",
        "!wget https://roboti.us/download/mujoco200_linux.zip\n",
        "\n",
        "!wget https://roboti.us/file/mjkey.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcXVniz_p4RN",
        "outputId": "c47e6f81-4c7f-4afc-9567-1706458820ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.mujoco’: File exists\n",
            "Archive:  mujoco200_linux.zip\n",
            "   creating: /root/.mujoco/mujoco200_linux/\n",
            "   creating: /root/.mujoco/mujoco200_linux/sample/\n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/mjxmake.m  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/makefile  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/mjx.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/simulate.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/record.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/basic.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/derivative.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/compile.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/testspeed.cpp  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/sample/testxml.cpp  \n",
            "   creating: /root/.mujoco/mujoco200_linux/model/\n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/sponge.png  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/softellipsoid.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/softcylinder.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/softbox.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/scene.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/rope.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/particle.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/marble.png  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/loop.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/grid2pin.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/grid2.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/grid1pin.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/grid1.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/cloth.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/carpet.png  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/hammock.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/humanoid100.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/humanoid.xml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/model/arm26.xml  \n",
            "   creating: /root/.mujoco/mujoco200_linux/include/\n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/uitools.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/uitools.c  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mujoco.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mjxmacro.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mjvisualize.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mjui.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mjrender.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mjmodel.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/mjdata.h  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/include/glfw3.h  \n",
            "   creating: /root/.mujoco/mujoco200_linux/doc/\n",
            "  inflating: /root/.mujoco/mujoco200_linux/doc/README.txt  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/doc/REFERENCE.txt  \n",
            "   creating: /root/.mujoco/mujoco200_linux/bin/\n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libmujoco200.so  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/simulate  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/record  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/basic  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/derivative  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/compile  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/testspeed  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/testxml  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libmujoco200nogl.so  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libglfw3.a  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libglfw.so.3  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libglewosmesa.so  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libglewegl.so  \n",
            "  inflating: /root/.mujoco/mujoco200_linux/bin/libglew.so  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!mkdir /root/.mujoco\n",
        "\n",
        "### mujoco 210\n",
        "#!tar -xf mujoco210-linux-x86_64.tar.gz -C /.mujoco/\n",
        "#!ls -alh /.mujoco/mujoco210\n",
        "\n",
        "### mujoco 200\n",
        "!unzip mujoco200_linux.zip -d /root/.mujoco/\n",
        "!cp -r /root/.mujoco/mujoco200_linux /root/.mujoco/mujoco200\n",
        "\n",
        "!mv mjkey.txt /root/.mujoco/\n",
        "\n",
        "!cp -r /root/.mujoco/mujoco200/bin/* /usr/lib/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X3JOM3RTcPO",
        "outputId": "fab50453-75c9-48fd-f957-c956d269b35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24K\n",
            "drwxr-xr-x 4 root root 4.0K Jul  5 11:49 .\n",
            "drwx------ 1 root root 4.0K Jul  5 11:33 ..\n",
            "-rw-r--r-- 1 root root  768 Oct 18  2021 mjkey.txt\n",
            "drwxr-xr-x 7 root root 4.0K Jul  5 11:49 mujoco200\n",
            "drwxrwxr-x 7 root root 4.0K Oct  2  2018 mujoco200_linux\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls -alh /root/.mujoco/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVQWcww_uZMo",
        "outputId": "f62b57de-2f7d-4a3f-b894-75d6af90b150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AviuDDxpqhOs",
        "outputId": "a941bc28-c5bd-4a84-8730-220d9467e1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mujoco_py==2.0.2.8 in /usr/local/lib/python3.10/dist-packages (2.0.2.8)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mujoco_py==2.0.2.8) (2.6.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from mujoco_py==2.0.2.8) (1.22.4)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py==2.0.2.8) (0.29.35)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py==2.0.2.8) (2.25.1)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco_py==2.0.2.8) (1.15.1)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.10/dist-packages (from mujoco_py==2.0.2.8) (0.18)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco_py==2.0.2.8) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco_py==2.0.2.8) (8.4.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###### mujoco-py setup ######\n",
        "\n",
        "!pip install mujoco_py==2.0.2.8\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duUbqfKEordx",
        "outputId": "b2fb6ca9-4bef-4ecb-c3d3-023c09f39775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'd4rl' already exists and is not an empty directory.\n",
            "Obtaining file:///content/d4rl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mjrl@ git+https://github.com/aravindr93/mjrl@master#egg=mjrl (from D4RL==1.1)\n",
            "  Cloning https://github.com/aravindr93/mjrl (to revision master) to /tmp/pip-install-ipmm4_l9/mjrl_802c72c805004bffb1c23b810ed2b9f1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/aravindr93/mjrl /tmp/pip-install-ipmm4_l9/mjrl_802c72c805004bffb1c23b810ed2b9f1\n",
            "  Resolved https://github.com/aravindr93/mjrl to commit 3871d93763d3b49c4741e6daeaebbc605fe140dc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym<0.24.0 in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (1.22.4)\n",
            "Requirement already satisfied: mujoco_py in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (2.0.2.8)\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (3.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (3.8.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (2.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (8.1.3)\n",
            "Requirement already satisfied: dm_control>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from D4RL==1.1) (1.0.13)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (1.4.0)\n",
            "Requirement already satisfied: dm-env in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (1.6)\n",
            "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (0.1.8)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (2.6.2)\n",
            "Requirement already satisfied: labmaze in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (1.0.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (4.9.2)\n",
            "Requirement already satisfied: mujoco>=2.3.6 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (2.3.6)\n",
            "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (3.20.3)\n",
            "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (3.1.7)\n",
            "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (2.27.1)\n",
            "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (67.7.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dm_control>=1.0.3->D4RL==1.1) (4.65.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0->D4RL==1.1) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.24.0->D4RL==1.1) (0.0.8)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->D4RL==1.1) (0.29.35)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->D4RL==1.1) (2.25.1)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->D4RL==1.1) (1.15.1)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.10/dist-packages (from mujoco_py->D4RL==1.1) (0.18)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco_py->D4RL==1.1) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco_py->D4RL==1.1) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->D4RL==1.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->D4RL==1.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->D4RL==1.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control>=1.0.3->D4RL==1.1) (3.4)\n",
            "Installing collected packages: D4RL\n",
            "  Attempting uninstall: D4RL\n",
            "    Found existing installation: D4RL 1.1\n",
            "    Uninstalling D4RL-1.1:\n",
            "      Successfully uninstalled D4RL-1.1\n",
            "  Running setup.py develop for D4RL\n",
            "Successfully installed D4RL-1.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###### D4RL setup ######\n",
        "\n",
        "## !pip uninstall dm_control==0.0.364896371\n",
        "\n",
        "!git clone https://github.com/rail-berkeley/d4rl.git\n",
        "\n",
        "### edit dm_control version in d4rl setup.py\n",
        "!sed -i \"s;dm_control @ git+git://github.com/deepmind/dm_control@master#egg=dm_control;dm_control==0.0.364896371;g\" /content/d4rl/setup.py\n",
        "\n",
        "### edit mjrl install in d4rl setup.py to use github's new https protocol instead of git SSH\n",
        "!sed -i \"s;mjrl @ git+git://github.com/aravindr93/mjrl@master#egg=mjrl;mjrl @ git+https://github.com/aravindr93/mjrl@master#egg=mjrl;g\" /content/d4rl/setup.py\n",
        "\n",
        "!pip install -e d4rl/.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVrmCbNMAwQk"
      },
      "outputs": [],
      "source": [
        "\n",
        "###### restart runtime ######\n",
        "\n",
        "exit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBD3fRknjEj6"
      },
      "source": [
        "# check mujoco-py and D4RL installation\n",
        "\n",
        "* if check fails then **Restart Runtime** again\n",
        "* if check still fails then Factory reset runtime and install again\n",
        "* After installing, first import will be slow as the lib will be built again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uycTGiqjKYK",
        "outputId": "ac3da6a9-0f16-4c7c-b0cc-888c0e84eba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
            "Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it depends on /usr/local/lib/python3.10/dist-packages/mujoco_py/pxd/mujoco.pxd.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:running build_ext\n",
            "INFO:root:building 'mujoco_py.cymj' extension\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n",
            "INFO:root:x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/root/.mujoco/mujoco200/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "INFO:root:x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/root/.mujoco/mujoco200/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n",
            "INFO:root:x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/root/.mujoco/mujoco200/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-R/root/.mujoco/mujoco200/bin -lmujoco200 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.8_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
            "No module named 'flow'\n",
            "INFO:absl:MUJOCO_GL is not set, so an OpenGL backend will be chosen automatically.\n",
            "/usr/local/lib/python3.10/dist-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
            "  warnings.warn(message, GLFWError)\n",
            "INFO:absl:Successfully imported OpenGL backend: glfw\n",
            "INFO:absl:MuJoCo library version is: 2.3.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mujoco-py check passed\n",
            "d4rl check passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
            "No module named 'carla'\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        }
      ],
      "source": [
        "# set mujoco env path if not already set\n",
        "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
        "\n",
        "import gym\n",
        "import d4rl # Import required to register environments\n",
        "\n",
        "\n",
        "env = gym.make('Walker2d-v3')\n",
        "env.reset()\n",
        "env.step(env.action_space.sample())\n",
        "env.close()\n",
        "print(\"mujoco-py check passed\")\n",
        "\n",
        "env = gym.make('walker2d-medium-v2')\n",
        "env.reset()\n",
        "env.step(env.action_space.sample())\n",
        "env.close()\n",
        "print(\"d4rl check passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TpGEYTblzQc"
      },
      "source": [
        "# download D4RL data\n",
        "\n",
        "*   skip this block if data is already downloaded\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31ELEKOih7D",
        "outputId": "20ef28cb-c2c6-49e9-bb9e-2c444f11800e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
            "./data\n",
            "processing:  walker2d-medium-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/walker2d_medium-v2.hdf5 to /root/.d4rl/datasets/walker2d_medium-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 21/21 [00:02<00:00,  9.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 999995\n",
            "Trajectory returns: mean = 2852.08837890625, std = 1095.443359375, max = 4226.93994140625, min = -6.6056718826293945\n",
            "processing:  walker2d-medium-expert-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/walker2d_medium_expert-v2.hdf5 to /root/.d4rl/datasets/walker2d_medium_expert-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 9/9 [00:06<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 1999209\n",
            "Trajectory returns: mean = 3796.57177734375, std = 1312.2825927734375, max = 5011.693359375, min = -6.6056718826293945\n",
            "processing:  walker2d-medium-replay-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/walker2d_medium_replay-v2.hdf5 to /root/.d4rl/datasets/walker2d_medium_replay-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 11/11 [00:00<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 302000\n",
            "Trajectory returns: mean = 682.7012939453125, std = 895.95556640625, max = 4132.00048828125, min = -50.196834564208984\n",
            "processing:  halfcheetah-medium-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/halfcheetah_medium-v2.hdf5 to /root/.d4rl/datasets/halfcheetah_medium-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 21/21 [00:03<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 1000000\n",
            "Trajectory returns: mean = 4770.3349609375, std = 355.7503967285156, max = 5309.37939453125, min = -310.23419189453125\n",
            "processing:  halfcheetah-medium-expert-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/halfcheetah_medium_expert-v2.hdf5 to /root/.d4rl/datasets/halfcheetah_medium_expert-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 9/9 [00:04<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 2000000\n",
            "Trajectory returns: mean = 7713.38037109375, std = 2970.242431640625, max = 11252.03515625, min = -310.23419189453125\n",
            "processing:  halfcheetah-medium-replay-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/halfcheetah_medium_replay-v2.hdf5 to /root/.d4rl/datasets/halfcheetah_medium_replay-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 11/11 [00:00<00:00, 22.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 202000\n",
            "Trajectory returns: mean = 3093.28564453125, std = 1680.6939697265625, max = 4985.1416015625, min = -638.4852905273438\n",
            "processing:  hopper-medium-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/hopper_medium-v2.hdf5 to /root/.d4rl/datasets/hopper_medium-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 21/21 [00:01<00:00, 10.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 999906\n",
            "Trajectory returns: mean = 1422.05615234375, std = 378.9537048339844, max = 3222.360595703125, min = 315.8680114746094\n",
            "processing:  hopper-medium-expert-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/hopper_medium_expert-v2.hdf5 to /root/.d4rl/datasets/hopper_medium_expert-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 9/9 [00:03<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 1999400\n",
            "Trajectory returns: mean = 2089.87841796875, std = 1039.9569091796875, max = 3759.083740234375, min = 315.8680114746094\n",
            "processing:  hopper-medium-replay-v2\n",
            "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/hopper_medium_replay-v2.hdf5 to /root/.d4rl/datasets/hopper_medium_replay-v2.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "load datafile: 100%|██████████| 11/11 [00:00<00:00, 12.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples collected: 402000\n",
            "Trajectory returns: mean = 467.3020324707031, std = 511.0256042480469, max = 3192.925048828125, min = -1.4400691986083984\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# set mujoco env path if not already set\n",
        "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
        "\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import collections\n",
        "import pickle\n",
        "\n",
        "import d4rl\n",
        "\n",
        "datasets = []\n",
        "\n",
        "data_dir = \"./data\"\n",
        "\n",
        "print(data_dir)\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "for env_name in ['walker2d', 'halfcheetah', 'hopper']:\n",
        "    for dataset_type in ['medium', 'medium-expert', 'medium-replay']:\n",
        "\n",
        "        name = f'{env_name}-{dataset_type}-v2'\n",
        "        pkl_file_path = os.path.join(data_dir, name)\n",
        "\n",
        "        print(\"processing: \", name)\n",
        "\n",
        "        env = gym.make(name)\n",
        "        dataset = env.get_dataset()\n",
        "\n",
        "        N = dataset['rewards'].shape[0]\n",
        "        data_ = collections.defaultdict(list)\n",
        "\n",
        "        use_timeouts = False\n",
        "        if 'timeouts' in dataset:\n",
        "            use_timeouts = True\n",
        "\n",
        "        episode_step = 0\n",
        "        paths = []\n",
        "        for i in range(N):\n",
        "            done_bool = bool(dataset['terminals'][i])\n",
        "            if use_timeouts:\n",
        "                final_timestep = dataset['timeouts'][i]\n",
        "            else:\n",
        "                final_timestep = (episode_step == 1000-1)\n",
        "            for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:\n",
        "                data_[k].append(dataset[k][i])\n",
        "            if done_bool or final_timestep:\n",
        "                episode_step = 0\n",
        "                episode_data = {}\n",
        "                for k in data_:\n",
        "                    episode_data[k] = np.array(data_[k])\n",
        "                paths.append(episode_data)\n",
        "                data_ = collections.defaultdict(list)\n",
        "            episode_step += 1\n",
        "\n",
        "        returns = np.array([np.sum(p['rewards']) for p in paths])\n",
        "        num_samples = np.sum([p['rewards'].shape[0] for p in paths])\n",
        "        print(f'Number of samples collected: {num_samples}')\n",
        "        print(f'Trajectory returns: mean = {np.mean(returns)}, std = {np.std(returns)}, max = {np.max(returns)}, min = {np.min(returns)}')\n",
        "\n",
        "        with open(f'{pkl_file_path}.pkl', 'wb') as f:\n",
        "            pickle.dump(paths, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHGzjOmbamJz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Nk5Gp7hUGA"
      },
      "source": [
        "# import libs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4xiijmBixUm",
        "outputId": "19960cc5-c813-4493-f557-3d2b7d4a361c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# set mujoco env path if not already set\n",
        "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import collections\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQcLNRgD6SaW"
      },
      "source": [
        "# training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdtDsvit6m_e",
        "outputId": "f2374224-5311-4205-bbdf-9245b38eda83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device set to:  cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = \"medium\"       # medium / medium-replay / medium-expert\n",
        "rtg_scale = 1000                # scale to normalize returns to go\n",
        "\n",
        "# use v3 env for evaluation because\n",
        "# DT paper evaluates results on v3 envs\n",
        "\n",
        "# env_name = 'Walker2d-v3'\n",
        "# rtg_target = 5000\n",
        "# env_d4rl_name = f'walker2d-{dataset}-v2'\n",
        "\n",
        "# env_name = 'HalfCheetah-v3'\n",
        "# rtg_target = 6000\n",
        "# env_d4rl_name = f'halfcheetah-{dataset}-v2'\n",
        "\n",
        "env_name = 'Hopper-v3'\n",
        "rtg_target = 3600\n",
        "env_d4rl_name = f'hopper-{dataset}-v2'\n",
        "\n",
        "\n",
        "max_eval_ep_len = 1000      # max len of one evaluation episode\n",
        "num_eval_ep = 10            # num of evaluation episodes per iteration\n",
        "\n",
        "batch_size = 64             # training batch size\n",
        "lr = 1e-4                   # learning rate\n",
        "wt_decay = 1e-4             # weight decay\n",
        "warmup_steps = 10000        # warmup steps for lr scheduler\n",
        "\n",
        "# total updates = max_train_iters x num_updates_per_iter\n",
        "max_train_iters = 200\n",
        "num_updates_per_iter = 100\n",
        "\n",
        "context_len = 20        # K in decision transformer\n",
        "n_blocks = 3            # num of transformer blocks\n",
        "embed_dim = 128         # embedding (hidden) dim of transformer\n",
        "n_heads = 1             # num of transformer heads\n",
        "dropout_p = 0.1         # dropout probability\n",
        "\n",
        "\n",
        "\n",
        "# load data from this file\n",
        "dataset_path = f'data/{env_d4rl_name}.pkl'\n",
        "\n",
        "# saves model and csv in this directory\n",
        "log_dir = \"./dt_runs/\"\n",
        "\n",
        "\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "# training and evaluation device\n",
        "device_name = 'cuda'\n",
        "device = torch.device(device_name)\n",
        "print(\"device set to: \", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJM0LG1iziA"
      },
      "source": [
        "# decision transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHMl_Y1SicXb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "this extremely minimal GPT model is based on:\n",
        "Misha Laskin's tweet:\n",
        "https://twitter.com/MishaLaskin/status/1481767788775628801?cxt=HHwWgoCzmYD9pZApAAAA\n",
        "\n",
        "and its corresponding notebook:\n",
        "https://colab.research.google.com/drive/1NUBqyboDcGte5qAJKOl8gaJC28V_73Iv?usp=sharing\n",
        "\n",
        "the above colab has a bug while applying masked_fill which is fixed in the\n",
        "following code\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class MaskedCausalAttention(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.max_T = max_T\n",
        "\n",
        "        self.q_net = nn.Linear(h_dim, h_dim)\n",
        "        self.k_net = nn.Linear(h_dim, h_dim)\n",
        "        self.v_net = nn.Linear(h_dim, h_dim)\n",
        "\n",
        "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "\n",
        "        self.att_drop = nn.Dropout(drop_p)\n",
        "        self.proj_drop = nn.Dropout(drop_p)\n",
        "\n",
        "        ones = torch.ones((max_T, max_T))\n",
        "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
        "\n",
        "        # register buffer makes sure mask does not get updated\n",
        "        # during backpropagation\n",
        "        self.register_buffer('mask',mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
        "\n",
        "        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
        "\n",
        "        # rearrange q, k, v as (B, N, T, D)\n",
        "        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
        "\n",
        "        # weights (B, N, T, T)\n",
        "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
        "        # causal mask applied to weights\n",
        "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
        "        # normalize weights, all -inf -> 0 after softmax\n",
        "        normalized_weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "        # attention (B, N, T, D)\n",
        "        attention = self.att_drop(normalized_weights @ v)\n",
        "\n",
        "        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
        "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
        "\n",
        "        out = self.proj_drop(self.proj_net(attention))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "        self.mlp = nn.Sequential(\n",
        "                nn.Linear(h_dim, 4*h_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(4*h_dim, h_dim),\n",
        "                nn.Dropout(drop_p),\n",
        "            )\n",
        "        self.ln1 = nn.LayerNorm(h_dim)\n",
        "        self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
        "        x = x + self.attention(x) # residual\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x) # residual\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecisionTransformer(nn.Module):\n",
        "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
        "                 n_heads, drop_p, max_timestep=4096):\n",
        "        super().__init__()\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.h_dim = h_dim\n",
        "\n",
        "        ### transformer blocks\n",
        "        input_seq_len = 3 * context_len\n",
        "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
        "        self.transformer = nn.Sequential(*blocks)\n",
        "\n",
        "        ### projection heads (project to embedding)\n",
        "        self.embed_ln = nn.LayerNorm(h_dim)\n",
        "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
        "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
        "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
        "\n",
        "        # # discrete actions\n",
        "        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
        "        # use_action_tanh = False # False for discrete actions\n",
        "\n",
        "        # continuous actions\n",
        "        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
        "        use_action_tanh = True # True for continuous actions\n",
        "\n",
        "        ### prediction heads\n",
        "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
        "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
        "        self.predict_action = nn.Sequential(\n",
        "            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, timesteps, states, actions, returns_to_go):\n",
        "\n",
        "        B, T, _ = states.shape\n",
        "\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "\n",
        "        # time embeddings are treated similar to positional embeddings\n",
        "        state_embeddings = self.embed_state(states) + time_embeddings\n",
        "        action_embeddings = self.embed_action(actions) + time_embeddings\n",
        "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
        "\n",
        "        # stack rtg, states and actions and reshape sequence as\n",
        "        # (r1, s1, a1, r2, s2, a2 ...)\n",
        "        h = torch.stack(\n",
        "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
        "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
        "\n",
        "        h = self.embed_ln(h)\n",
        "\n",
        "        # transformer and prediction\n",
        "        h = self.transformer(h)\n",
        "\n",
        "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
        "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
        "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
        "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
        "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # get predictions\n",
        "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
        "        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n",
        "        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n",
        "\n",
        "        return state_preds, action_preds, return_preds\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision LSTM"
      ],
      "metadata": {
        "id": "kV087ILX68Lb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNizko0UfhbC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "LSTM Model\n",
        "\"\"\"\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class LstmModel(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, drop_p):\n",
        "        super().__init__()\n",
        "        self.hidden_size = h_dim\n",
        "        self.lstm_cell = nn.LSTM(h_dim, h_dim)\n",
        "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "        self.proj_drop = nn.Dropout(drop_p)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
        "\n",
        "        # rearrange q, k, v as (B, N, T, D)\n",
        "        #x, _ = self.lstm_cell(x)\n",
        "        # extract only the last time step\n",
        "        #x = x[:, -1, :]\n",
        "\n",
        "        h_0 = Variable(torch.zeros(1, T, self.hidden_size)).cuda() #hidden state\n",
        "        c_0 = Variable(torch.zeros(1, T, self.hidden_size)).cuda() #internal state\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm_cell(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.proj_drop(self.proj_net(hn))\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, drop_p):\n",
        "        super().__init__()\n",
        "        self.lstm = LstmModel(h_dim, max_T, drop_p)  # Input dim is 3, output dim is 3\n",
        "        #MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "        self.mlp = nn.Sequential(\n",
        "                nn.Linear(h_dim, 4*h_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(4*h_dim, h_dim),\n",
        "                nn.Dropout(drop_p),\n",
        "            )\n",
        "        self.ln1 = nn.LayerNorm(h_dim)\n",
        "        self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
        "        a = self.lstm(x)\n",
        "        x = x + self.lstm(x) # residual\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x) # residual\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecisionLstm(nn.Module):\n",
        "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
        "                 drop_p, max_timestep=4096):\n",
        "        super().__init__()\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.h_dim = h_dim\n",
        "\n",
        "        ### lstm blocks\n",
        "        input_seq_len = 3 * context_len\n",
        "        blocks = [Block(h_dim, input_seq_len, drop_p) for _ in range(n_blocks)]\n",
        "        self.lstm = nn.Sequential(*blocks)\n",
        "\n",
        "        ### projection heads (project to embedding)\n",
        "        self.embed_ln = nn.LayerNorm(h_dim)\n",
        "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
        "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
        "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
        "\n",
        "        # # discrete actions\n",
        "        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
        "        # use_action_tanh = False # False for discrete actions\n",
        "\n",
        "        # continuous actions\n",
        "        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
        "        use_action_tanh = True # True for continuous actions\n",
        "\n",
        "        ### prediction heads\n",
        "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
        "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
        "        self.predict_action = nn.Sequential(\n",
        "            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, timesteps, states, actions, returns_to_go):\n",
        "\n",
        "        B, T, _ = states.shape\n",
        "\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "\n",
        "        # time embeddings are treated similar to positional embeddings\n",
        "        state_embeddings = self.embed_state(states) + time_embeddings\n",
        "        action_embeddings = self.embed_action(actions) + time_embeddings\n",
        "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
        "\n",
        "        # stack rtg, states and actions and reshape sequence as\n",
        "        # (r1, s1, a1, r2, s2, a2 ...)\n",
        "        h = torch.stack(\n",
        "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
        "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
        "\n",
        "        h = self.embed_ln(h)\n",
        "\n",
        "        # lstm and prediction\n",
        "        h = self.lstm(h)\n",
        "\n",
        "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
        "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
        "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
        "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
        "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # get predictions\n",
        "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
        "        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n",
        "        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n",
        "\n",
        "        return state_preds, action_preds, return_preds\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLHjV3q28LNr"
      },
      "source": [
        "# infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btnq_IL_j4PO"
      },
      "outputs": [],
      "source": [
        "\n",
        "## from infos.py from official d4rl github repo\n",
        "\n",
        "REF_MAX_SCORE = {\n",
        "    'halfcheetah' : 12135.0,\n",
        "    'walker2d' : 4592.3,\n",
        "    'hopper' : 3234.3,\n",
        "}\n",
        "\n",
        "REF_MIN_SCORE = {\n",
        "    'halfcheetah' : -280.178953,\n",
        "    'walker2d' : 1.629008,\n",
        "    'hopper' : -20.272305,\n",
        "}\n",
        "\n",
        "\n",
        "## calculated from d4rl datasets\n",
        "\n",
        "D4RL_DATASET_STATS = {\n",
        "        'halfcheetah-medium-v2': {\n",
        "                'state_mean':[-0.06845773756504059, 0.016414547339081764, -0.18354906141757965,\n",
        "                              -0.2762460708618164, -0.34061527252197266, -0.09339715540409088,\n",
        "                              -0.21321271359920502, -0.0877423882484436, 5.173007488250732,\n",
        "                              -0.04275195300579071, -0.036108363419771194, 0.14053793251514435,\n",
        "                              0.060498327016830444, 0.09550975263118744, 0.06739100068807602,\n",
        "                              0.005627387668937445, 0.013382787816226482\n",
        "                ],\n",
        "                'state_std':[0.07472999393939972, 0.3023499846458435, 0.30207309126853943,\n",
        "                             0.34417077898979187, 0.17619241774082184, 0.507205605506897,\n",
        "                             0.2567007839679718, 0.3294812738895416, 1.2574149370193481,\n",
        "                             0.7600541710853577, 1.9800915718078613, 6.565362453460693,\n",
        "                             7.466367721557617, 4.472222805023193, 10.566964149475098,\n",
        "                             5.671932697296143, 7.4982590675354\n",
        "                ]\n",
        "            },\n",
        "        'halfcheetah-medium-replay-v2': {\n",
        "                'state_mean':[-0.12880703806877136, 0.3738119602203369, -0.14995987713336945,\n",
        "                              -0.23479078710079193, -0.2841278612613678, -0.13096535205841064,\n",
        "                              -0.20157982409000397, -0.06517726927995682, 3.4768247604370117,\n",
        "                              -0.02785065770149231, -0.015035249292850494, 0.07697279006242752,\n",
        "                              0.01266712136566639, 0.027325302362442017, 0.02316424623131752,\n",
        "                              0.010438721626996994, -0.015839405357837677\n",
        "                ],\n",
        "                'state_std':[0.17019015550613403, 1.284424901008606, 0.33442774415016174,\n",
        "                             0.3672759234905243, 0.26092398166656494, 0.4784106910228729,\n",
        "                             0.3181420564651489, 0.33552637696266174, 2.0931615829467773,\n",
        "                             0.8037433624267578, 1.9044333696365356, 6.573209762573242,\n",
        "                             7.572863578796387, 5.069749355316162, 9.10555362701416,\n",
        "                             6.085654258728027, 7.25300407409668\n",
        "                ]\n",
        "            },\n",
        "        'halfcheetah-medium-expert-v2': {\n",
        "                'state_mean':[-0.05667462572455406, 0.024369969964027405, -0.061670560389757156,\n",
        "                              -0.22351515293121338, -0.2675151228904724, -0.07545716315507889,\n",
        "                              -0.05809682980179787, -0.027675075456500053, 8.110626220703125,\n",
        "                              -0.06136331334710121, -0.17986927926540375, 0.25175222754478455,\n",
        "                              0.24186332523822784, 0.2519369423389435, 0.5879552960395813,\n",
        "                              -0.24090635776519775, -0.030184272676706314\n",
        "                ],\n",
        "                'state_std':[0.06103534251451492, 0.36054104566574097, 0.45544400811195374,\n",
        "                             0.38476887345314026, 0.2218363732099533, 0.5667523741722107,\n",
        "                             0.3196682929992676, 0.2852923572063446, 3.443821907043457,\n",
        "                             0.6728139519691467, 1.8616976737976074, 9.575807571411133,\n",
        "                             10.029894828796387, 5.903450012207031, 12.128185272216797,\n",
        "                             6.4811787605285645, 6.378620147705078\n",
        "                ]\n",
        "            },\n",
        "        'walker2d-medium-v2': {\n",
        "                'state_mean':[1.218966007232666, 0.14163373410701752, -0.03704913705587387,\n",
        "                              -0.13814310729503632, 0.5138224363327026, -0.04719110205769539,\n",
        "                              -0.47288352251052856, 0.042254164814949036, 2.3948874473571777,\n",
        "                              -0.03143199160695076, 0.04466355964541435, -0.023907244205474854,\n",
        "                              -0.1013401448726654, 0.09090937674045563, -0.004192637279629707,\n",
        "                              -0.12120571732521057, -0.5497063994407654\n",
        "                ],\n",
        "                'state_std':[0.12311358004808426, 0.3241879940032959, 0.11456084251403809,\n",
        "                             0.2623065710067749, 0.5640279054641724, 0.2271878570318222,\n",
        "                             0.3837319612503052, 0.7373676896095276, 1.2387926578521729,\n",
        "                             0.798020601272583, 1.5664079189300537, 1.8092705011367798,\n",
        "                             3.025604248046875, 4.062486171722412, 1.4586567878723145,\n",
        "                             3.7445690631866455, 5.5851287841796875\n",
        "                ]\n",
        "            },\n",
        "        'walker2d-medium-replay-v2': {\n",
        "                'state_mean':[1.209364652633667, 0.13264022767543793, -0.14371201395988464,\n",
        "                              -0.2046516090631485, 0.5577612519264221, -0.03231537342071533,\n",
        "                              -0.2784661054611206, 0.19130706787109375, 1.4701707363128662,\n",
        "                              -0.12504704296588898, 0.0564953051507473, -0.09991033375263214,\n",
        "                              -0.340340256690979, 0.03546293452382088, -0.08934258669614792,\n",
        "                              -0.2992438077926636, -0.5984178185462952\n",
        "                ],\n",
        "                'state_std':[0.11929835379123688, 0.3562574088573456, 0.25852200388908386,\n",
        "                             0.42075422406196594, 0.5202291011810303, 0.15685082972049713,\n",
        "                             0.36770978569984436, 0.7161387801170349, 1.3763766288757324,\n",
        "                             0.8632221817970276, 2.6364643573760986, 3.0134117603302,\n",
        "                             3.720684051513672, 4.867283821105957, 2.6681625843048096,\n",
        "                             3.845186948776245, 5.4768385887146\n",
        "                ]\n",
        "            },\n",
        "        'walker2d-medium-expert-v2': {\n",
        "                'state_mean':[1.2294334173202515, 0.16869689524173737, -0.07089081406593323,\n",
        "                              -0.16197483241558075, 0.37101927399635315, -0.012209027074277401,\n",
        "                              -0.42461398243904114, 0.18986578285694122, 3.162475109100342,\n",
        "                              -0.018092676997184753, 0.03496946766972542, -0.013921679928898811,\n",
        "                              -0.05937029421329498, -0.19549426436424255, -0.0019200450042262673,\n",
        "                              -0.062483321875333786, -0.27366524934768677\n",
        "                ],\n",
        "                'state_std':[0.09932824969291687, 0.25981399416923523, 0.15062759816646576,\n",
        "                             0.24249176681041718, 0.6758718490600586, 0.1650741547346115,\n",
        "                             0.38140663504600525, 0.6962361335754395, 1.3501490354537964,\n",
        "                             0.7641991376876831, 1.534574270248413, 2.1785972118377686,\n",
        "                             3.276582717895508, 4.766193866729736, 1.1716983318328857,\n",
        "                             4.039782524108887, 5.891613960266113\n",
        "                ]\n",
        "            },\n",
        "        'hopper-medium-v2': {\n",
        "                'state_mean':[1.311279058456421, -0.08469521254301071, -0.5382719039916992,\n",
        "                              -0.07201576232910156, 0.04932365566492081, 2.1066856384277344,\n",
        "                              -0.15017354488372803, 0.008783451281487942, -0.2848185896873474,\n",
        "                              -0.18540096282958984, -0.28461286425590515\n",
        "                ],\n",
        "                'state_std':[0.17790751159191132, 0.05444620922207832, 0.21297138929367065,\n",
        "                             0.14530418813228607, 0.6124444007873535, 0.8517446517944336,\n",
        "                             1.4515252113342285, 0.6751695871353149, 1.5362390279769897,\n",
        "                             1.616074562072754, 5.607253551483154\n",
        "                ]\n",
        "            },\n",
        "        'hopper-medium-replay-v2': {\n",
        "                'state_mean':[1.2305138111114502, -0.04371410980820656, -0.44542956352233887,\n",
        "                              -0.09370097517967224, 0.09094487875699997, 1.3694725036621094,\n",
        "                              -0.19992674887180328, -0.022861352190375328, -0.5287045240402222,\n",
        "                              -0.14465883374214172, -0.19652697443962097\n",
        "                ],\n",
        "                'state_std':[0.1756512075662613, 0.0636928603053093, 0.3438323438167572,\n",
        "                             0.19566889107227325, 0.5547984838485718, 1.051029920578003,\n",
        "                             1.158307671546936, 0.7963128685951233, 1.4802359342575073,\n",
        "                             1.6540331840515137, 5.108601093292236\n",
        "                ]\n",
        "            },\n",
        "        'hopper-medium-expert-v2': {\n",
        "                'state_mean':[1.3293815851211548, -0.09836531430482864, -0.5444297790527344,\n",
        "                              -0.10201650857925415, 0.02277466468513012, 2.3577215671539307,\n",
        "                              -0.06349576264619827, -0.00374026270583272, -0.1766270101070404,\n",
        "                              -0.11862941086292267, -0.12097819894552231\n",
        "                ],\n",
        "                'state_std':[0.17012375593185425, 0.05159067362546921, 0.18141433596611023,\n",
        "                             0.16430604457855225, 0.6023368239402771, 0.7737284898757935,\n",
        "                             1.4986555576324463, 0.7483318448066711, 1.7953159809112549,\n",
        "                             2.0530025959014893, 5.725032806396484\n",
        "                ]\n",
        "            },\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pewE01Ca4BG0"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaaymCHPlynF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def discount_cumsum(x, gamma):\n",
        "    disc_cumsum = np.zeros_like(x)\n",
        "    disc_cumsum[-1] = x[-1]\n",
        "    for t in reversed(range(x.shape[0]-1)):\n",
        "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
        "    return disc_cumsum\n",
        "\n",
        "\n",
        "def get_d4rl_dataset_stats(env_d4rl_name):\n",
        "    return D4RL_DATASET_STATS[env_d4rl_name]\n",
        "\n",
        "\n",
        "def get_d4rl_normalized_score(score, env_name):\n",
        "    env_key = env_name.split('-')[0].lower()\n",
        "    assert env_key in REF_MAX_SCORE, f'no reference score for {env_key} env to calculate d4rl score'\n",
        "    return (score - REF_MIN_SCORE[env_key]) / (REF_MAX_SCORE[env_key] - REF_MIN_SCORE[env_key])\n",
        "\n",
        "\n",
        "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
        "                    num_eval_ep=10, max_test_ep_len=1000,\n",
        "                    state_mean=None, state_std=None, render=False):\n",
        "\n",
        "    eval_batch_size = 1  # required for forward pass\n",
        "\n",
        "    results = {}\n",
        "    total_reward = 0\n",
        "    total_timesteps = 0\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "\n",
        "    if state_mean is None:\n",
        "        state_mean = torch.zeros((state_dim,)).to(device)\n",
        "    else:\n",
        "        state_mean = torch.from_numpy(state_mean).to(device)\n",
        "\n",
        "    if state_std is None:\n",
        "        state_std = torch.ones((state_dim,)).to(device)\n",
        "    else:\n",
        "        state_std = torch.from_numpy(state_std).to(device)\n",
        "\n",
        "    # same as timesteps used for training the transformer\n",
        "    # also, crashes if device is passed to arange()\n",
        "    timesteps = torch.arange(start=0, end=max_test_ep_len, step=1)\n",
        "    timesteps = timesteps.repeat(eval_batch_size, 1).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _ in range(num_eval_ep):\n",
        "\n",
        "            # zeros place holders\n",
        "            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim),\n",
        "                                dtype=torch.float32, device=device)\n",
        "\n",
        "            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim),\n",
        "                                dtype=torch.float32, device=device)\n",
        "\n",
        "            rewards_to_go = torch.zeros((eval_batch_size, max_test_ep_len, 1),\n",
        "                                dtype=torch.float32, device=device)\n",
        "\n",
        "            # init episode\n",
        "            running_state = env.reset()\n",
        "            running_reward = 0\n",
        "            running_rtg = rtg_target / rtg_scale\n",
        "\n",
        "            for t in range(max_test_ep_len):\n",
        "\n",
        "                total_timesteps += 1\n",
        "\n",
        "                # add state in placeholder and normalize\n",
        "                states[0, t] = torch.from_numpy(running_state).to(device)\n",
        "                states[0, t] = (states[0, t] - state_mean) / state_std\n",
        "\n",
        "                # calcualate running rtg and add in placeholder\n",
        "                running_rtg = running_rtg - (running_reward / rtg_scale)\n",
        "                rewards_to_go[0, t] = running_rtg\n",
        "\n",
        "                if t < context_len:\n",
        "                    _, act_preds, _ = model.forward(timesteps[:,:context_len],\n",
        "                                                states[:,:context_len],\n",
        "                                                actions[:,:context_len],\n",
        "                                                rewards_to_go[:,:context_len])\n",
        "                    act = act_preds[0, t].detach()\n",
        "                else:\n",
        "                    _, act_preds, _ = model.forward(timesteps[:,t-context_len+1:t+1],\n",
        "                                                states[:,t-context_len+1:t+1],\n",
        "                                                actions[:,t-context_len+1:t+1],\n",
        "                                                rewards_to_go[:,t-context_len+1:t+1])\n",
        "                    act = act_preds[0, -1].detach()\n",
        "\n",
        "\n",
        "                running_state, running_reward, done, _ = env.step(act.cpu().numpy())\n",
        "\n",
        "                # add action in placeholder\n",
        "                actions[0, t] = act\n",
        "\n",
        "                total_reward += running_reward\n",
        "\n",
        "                if render:\n",
        "                    env.render()\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "    results['eval/avg_reward'] = total_reward / num_eval_ep\n",
        "    results['eval/avg_ep_len'] = total_timesteps / num_eval_ep\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDk9X9jJ8iAZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXXrs_PjAHrN"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Vb5rY_iiME",
        "outputId": "a8a0e8e9-09f1-4701-f724-0069f470bc6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/hopper-medium-v2.pkl\n",
            "num of trajectories in dataset:  2186\n",
            "minimum trajectory length in dataset:  145\n",
            "state mean:  [1.311279058456421, -0.08469521254301071, -0.5382719039916992, -0.07201576232910156, 0.04932365566492081, 2.1066856384277344, -0.15017354488372803, 0.008783451281487942, -0.2848185896873474, -0.18540096282958984, -0.28461286425590515]\n",
            "state std:  [0.17790751159191132, 0.05444620922207832, 0.21297138929367065, 0.14530418813228607, 0.6124444007873535, 0.8517446517944336, 1.4515252113342285, 0.6751695871353149, 1.5362390279769897, 1.616074562072754, 5.607253551483154]\n",
            "is state mean info correct:  True\n",
            "is state std info correct:  True\n"
          ]
        }
      ],
      "source": [
        "## check data\n",
        "\n",
        "# load dataset\n",
        "with open(dataset_path, 'rb') as f:\n",
        "    trajectories = pickle.load(f)\n",
        "\n",
        "min_len = 10**4\n",
        "states = []\n",
        "for traj in trajectories:\n",
        "    min_len = min(min_len, traj['observations'].shape[0])\n",
        "    states.append(traj['observations'])\n",
        "\n",
        "# used for input normalization\n",
        "states = np.concatenate(states, axis=0)\n",
        "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
        "\n",
        "print(dataset_path)\n",
        "print(\"num of trajectories in dataset: \", len(trajectories))\n",
        "print(\"minimum trajectory length in dataset: \", min_len)\n",
        "print(\"state mean: \", state_mean.tolist())\n",
        "print(\"state std: \", state_std.tolist())\n",
        "\n",
        "\n",
        "## check if info is correct\n",
        "print(\"is state mean info correct: \", state_mean.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_mean'])\n",
        "print(\"is state std info correct: \", state_std.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_std'])\n",
        "\n",
        "\n",
        "assert state_mean.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_mean']\n",
        "assert state_std.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_std']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo4zPTjjn0Qr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class D4RLTrajectoryDataset(Dataset):\n",
        "    def __init__(self, dataset_path, context_len, rtg_scale):\n",
        "\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # load dataset\n",
        "        with open(dataset_path, 'rb') as f:\n",
        "            self.trajectories = pickle.load(f)\n",
        "\n",
        "        # calculate min len of traj, state mean and variance\n",
        "        # and returns_to_go for all traj\n",
        "        min_len = 10**6\n",
        "        states = []\n",
        "        for traj in self.trajectories:\n",
        "            traj_len = traj['observations'].shape[0]\n",
        "            min_len = min(min_len, traj_len)\n",
        "            states.append(traj['observations'])\n",
        "            # calculate returns to go and rescale them\n",
        "            traj['returns_to_go'] = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
        "\n",
        "        # used for input normalization\n",
        "        states = np.concatenate(states, axis=0)\n",
        "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
        "\n",
        "        # normalize states\n",
        "        for traj in self.trajectories:\n",
        "            traj['observations'] = (traj['observations'] - self.state_mean) / self.state_std\n",
        "\n",
        "\n",
        "    def get_state_stats(self):\n",
        "        return self.state_mean, self.state_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        traj = self.trajectories[idx]\n",
        "        traj_len = traj['observations'].shape[0]\n",
        "\n",
        "        if traj_len >= self.context_len:\n",
        "            # sample random index to slice trajectory\n",
        "            si = random.randint(0, traj_len - self.context_len)\n",
        "\n",
        "            states = torch.from_numpy(traj['observations'][si : si + self.context_len])\n",
        "            actions = torch.from_numpy(traj['actions'][si : si + self.context_len])\n",
        "            returns_to_go = torch.from_numpy(traj['returns_to_go'][si : si + self.context_len])\n",
        "            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
        "\n",
        "            # all ones since no padding\n",
        "            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
        "\n",
        "        else:\n",
        "            padding_len = self.context_len - traj_len\n",
        "\n",
        "            # padding with zeros\n",
        "            states = torch.from_numpy(traj['observations'])\n",
        "            states = torch.cat([states,\n",
        "                                torch.zeros(([padding_len] + list(states.shape[1:])),\n",
        "                                dtype=states.dtype)],\n",
        "                               dim=0)\n",
        "\n",
        "            actions = torch.from_numpy(traj['actions'])\n",
        "            actions = torch.cat([actions,\n",
        "                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
        "                                dtype=actions.dtype)],\n",
        "                               dim=0)\n",
        "\n",
        "            returns_to_go = torch.from_numpy(traj['returns_to_go'])\n",
        "            returns_to_go = torch.cat([returns_to_go,\n",
        "                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
        "                                dtype=returns_to_go.dtype)],\n",
        "                               dim=0)\n",
        "\n",
        "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
        "\n",
        "            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long),\n",
        "                                   torch.zeros(padding_len, dtype=torch.long)],\n",
        "                                  dim=0)\n",
        "\n",
        "        return  timesteps, states, actions, returns_to_go, traj_mask\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7AK6T9Picu-"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RBLRM5nOVR_8",
        "outputId": "55ad1b87-7aa8-4623-859d-dba254498a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "start time: 23-07-05-12-16-25\n",
            "============================================================\n",
            "device set to: cuda\n",
            "dataset path: data/hopper-medium-v2.pkl\n",
            "model save path: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "log csv save path: ./dt_runs/dt_hopper-medium-v2_log_23-07-05-12-16-25.csv\n",
            "============================================================\n",
            "time elapsed: 0:00:07\n",
            "num of updates: 100\n",
            "action loss: 0.63921\n",
            "eval avg reward: 9.25628\n",
            "eval avg ep len: 11.40000\n",
            "eval d4rl score: 0.90730\n",
            "max d4rl score: -1.00000\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:00:09\n",
            "num of updates: 200\n",
            "action loss: 0.61183\n",
            "eval avg reward: 10.44446\n",
            "eval avg ep len: 12.10000\n",
            "eval d4rl score: 0.94380\n",
            "max d4rl score: 0.90730\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:00:12\n",
            "num of updates: 300\n",
            "action loss: 0.55818\n",
            "eval avg reward: 12.88339\n",
            "eval avg ep len: 14.00000\n",
            "eval d4rl score: 1.01874\n",
            "max d4rl score: 0.94380\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:00:16\n",
            "num of updates: 400\n",
            "action loss: 0.49662\n",
            "eval avg reward: 18.27812\n",
            "eval avg ep len: 17.00000\n",
            "eval d4rl score: 1.18450\n",
            "max d4rl score: 1.01874\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:00:23\n",
            "num of updates: 500\n",
            "action loss: 0.43743\n",
            "eval avg reward: 181.94996\n",
            "eval avg ep len: 126.50000\n",
            "eval d4rl score: 6.21348\n",
            "max d4rl score: 1.18450\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:00:38\n",
            "num of updates: 600\n",
            "action loss: 0.38574\n",
            "eval avg reward: 517.55772\n",
            "eval avg ep len: 239.50000\n",
            "eval d4rl score: 16.52537\n",
            "max d4rl score: 6.21348\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:00:53\n",
            "num of updates: 700\n",
            "action loss: 0.34977\n",
            "eval avg reward: 491.31873\n",
            "eval avg ep len: 219.70000\n",
            "eval d4rl score: 15.71915\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:01:05\n",
            "num of updates: 800\n",
            "action loss: 0.31607\n",
            "eval avg reward: 488.60528\n",
            "eval avg ep len: 206.30000\n",
            "eval d4rl score: 15.63577\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:01:16\n",
            "num of updates: 900\n",
            "action loss: 0.29033\n",
            "eval avg reward: 498.12124\n",
            "eval avg ep len: 205.30000\n",
            "eval d4rl score: 15.92816\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:01:28\n",
            "num of updates: 1000\n",
            "action loss: 0.27498\n",
            "eval avg reward: 440.93319\n",
            "eval avg ep len: 185.60000\n",
            "eval d4rl score: 14.17100\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:01:37\n",
            "num of updates: 1100\n",
            "action loss: 0.26170\n",
            "eval avg reward: 287.10039\n",
            "eval avg ep len: 149.80000\n",
            "eval d4rl score: 9.44433\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:01:49\n",
            "num of updates: 1200\n",
            "action loss: 0.24967\n",
            "eval avg reward: 507.02583\n",
            "eval avg ep len: 203.10000\n",
            "eval d4rl score: 16.20176\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:02:00\n",
            "num of updates: 1300\n",
            "action loss: 0.24147\n",
            "eval avg reward: 368.87452\n",
            "eval avg ep len: 165.10000\n",
            "eval d4rl score: 11.95693\n",
            "max d4rl score: 16.52537\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:02:14\n",
            "num of updates: 1400\n",
            "action loss: 0.23327\n",
            "eval avg reward: 677.19151\n",
            "eval avg ep len: 245.60000\n",
            "eval d4rl score: 21.43028\n",
            "max d4rl score: 16.52537\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:02:27\n",
            "num of updates: 1500\n",
            "action loss: 0.22473\n",
            "eval avg reward: 702.65121\n",
            "eval avg ep len: 249.40000\n",
            "eval d4rl score: 22.21255\n",
            "max d4rl score: 21.43028\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:02:44\n",
            "num of updates: 1600\n",
            "action loss: 0.21822\n",
            "eval avg reward: 833.97159\n",
            "eval avg ep len: 297.10000\n",
            "eval d4rl score: 26.24750\n",
            "max d4rl score: 22.21255\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:03:00\n",
            "num of updates: 1700\n",
            "action loss: 0.21244\n",
            "eval avg reward: 769.35713\n",
            "eval avg ep len: 266.10000\n",
            "eval d4rl score: 24.26216\n",
            "max d4rl score: 26.24750\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:03:17\n",
            "num of updates: 1800\n",
            "action loss: 0.20783\n",
            "eval avg reward: 903.67257\n",
            "eval avg ep len: 310.30000\n",
            "eval d4rl score: 28.38913\n",
            "max d4rl score: 26.24750\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:03:34\n",
            "num of updates: 1900\n",
            "action loss: 0.20327\n",
            "eval avg reward: 871.56843\n",
            "eval avg ep len: 301.70000\n",
            "eval d4rl score: 27.40270\n",
            "max d4rl score: 28.38913\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:03:51\n",
            "num of updates: 2000\n",
            "action loss: 0.19790\n",
            "eval avg reward: 879.99835\n",
            "eval avg ep len: 295.50000\n",
            "eval d4rl score: 27.66172\n",
            "max d4rl score: 28.38913\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:04:08\n",
            "num of updates: 2100\n",
            "action loss: 0.19452\n",
            "eval avg reward: 880.04294\n",
            "eval avg ep len: 298.20000\n",
            "eval d4rl score: 27.66309\n",
            "max d4rl score: 28.38913\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:04:22\n",
            "num of updates: 2200\n",
            "action loss: 0.19018\n",
            "eval avg reward: 553.61952\n",
            "eval avg ep len: 217.50000\n",
            "eval d4rl score: 17.63340\n",
            "max d4rl score: 28.38913\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:04:40\n",
            "num of updates: 2300\n",
            "action loss: 0.18695\n",
            "eval avg reward: 1009.87848\n",
            "eval avg ep len: 330.70000\n",
            "eval d4rl score: 31.65242\n",
            "max d4rl score: 28.38913\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:04:57\n",
            "num of updates: 2400\n",
            "action loss: 0.18391\n",
            "eval avg reward: 913.38043\n",
            "eval avg ep len: 302.10000\n",
            "eval d4rl score: 28.68742\n",
            "max d4rl score: 31.65242\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:05:16\n",
            "num of updates: 2500\n",
            "action loss: 0.18208\n",
            "eval avg reward: 1036.73722\n",
            "eval avg ep len: 334.00000\n",
            "eval d4rl score: 32.47768\n",
            "max d4rl score: 31.65242\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:05:33\n",
            "num of updates: 2600\n",
            "action loss: 0.17842\n",
            "eval avg reward: 966.17018\n",
            "eval avg ep len: 315.00000\n",
            "eval d4rl score: 30.30944\n",
            "max d4rl score: 32.47768\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:05:53\n",
            "num of updates: 2700\n",
            "action loss: 0.17594\n",
            "eval avg reward: 1086.48636\n",
            "eval avg ep len: 355.00000\n",
            "eval d4rl score: 34.00627\n",
            "max d4rl score: 32.47768\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:06:12\n",
            "num of updates: 2800\n",
            "action loss: 0.17248\n",
            "eval avg reward: 1053.00770\n",
            "eval avg ep len: 348.50000\n",
            "eval d4rl score: 32.97761\n",
            "max d4rl score: 34.00627\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:06:33\n",
            "num of updates: 2900\n",
            "action loss: 0.17099\n",
            "eval avg reward: 1162.77346\n",
            "eval avg ep len: 378.40000\n",
            "eval d4rl score: 36.35027\n",
            "max d4rl score: 34.00627\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:06:52\n",
            "num of updates: 3000\n",
            "action loss: 0.16900\n",
            "eval avg reward: 955.42680\n",
            "eval avg ep len: 337.60000\n",
            "eval d4rl score: 29.97933\n",
            "max d4rl score: 36.35027\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:07:13\n",
            "num of updates: 3100\n",
            "action loss: 0.16693\n",
            "eval avg reward: 1172.29220\n",
            "eval avg ep len: 384.90000\n",
            "eval d4rl score: 36.64274\n",
            "max d4rl score: 36.35027\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:07:35\n",
            "num of updates: 3200\n",
            "action loss: 0.16301\n",
            "eval avg reward: 1225.11925\n",
            "eval avg ep len: 395.80000\n",
            "eval d4rl score: 38.26591\n",
            "max d4rl score: 36.64274\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:07:55\n",
            "num of updates: 3300\n",
            "action loss: 0.16149\n",
            "eval avg reward: 1116.12253\n",
            "eval avg ep len: 372.90000\n",
            "eval d4rl score: 34.91687\n",
            "max d4rl score: 38.26591\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:08:16\n",
            "num of updates: 3400\n",
            "action loss: 0.16022\n",
            "eval avg reward: 1129.60553\n",
            "eval avg ep len: 385.40000\n",
            "eval d4rl score: 35.33115\n",
            "max d4rl score: 38.26591\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:08:42\n",
            "num of updates: 3500\n",
            "action loss: 0.15895\n",
            "eval avg reward: 1476.86305\n",
            "eval avg ep len: 478.20000\n",
            "eval d4rl score: 46.00099\n",
            "max d4rl score: 38.26591\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:09:07\n",
            "num of updates: 3600\n",
            "action loss: 0.15585\n",
            "eval avg reward: 1417.74601\n",
            "eval avg ep len: 469.10000\n",
            "eval d4rl score: 44.18456\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:09:29\n",
            "num of updates: 3700\n",
            "action loss: 0.15242\n",
            "eval avg reward: 1221.82464\n",
            "eval avg ep len: 399.00000\n",
            "eval d4rl score: 38.16468\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:09:50\n",
            "num of updates: 3800\n",
            "action loss: 0.15073\n",
            "eval avg reward: 1186.94976\n",
            "eval avg ep len: 390.60000\n",
            "eval d4rl score: 37.09311\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:10:15\n",
            "num of updates: 3900\n",
            "action loss: 0.14881\n",
            "eval avg reward: 1402.60725\n",
            "eval avg ep len: 448.00000\n",
            "eval d4rl score: 43.71940\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:10:36\n",
            "num of updates: 4000\n",
            "action loss: 0.14837\n",
            "eval avg reward: 1186.25826\n",
            "eval avg ep len: 397.50000\n",
            "eval d4rl score: 37.07186\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:10:59\n",
            "num of updates: 4100\n",
            "action loss: 0.14636\n",
            "eval avg reward: 1185.88289\n",
            "eval avg ep len: 400.10000\n",
            "eval d4rl score: 37.06033\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:11:23\n",
            "num of updates: 4200\n",
            "action loss: 0.14364\n",
            "eval avg reward: 1419.26149\n",
            "eval avg ep len: 465.80000\n",
            "eval d4rl score: 44.23112\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:11:46\n",
            "num of updates: 4300\n",
            "action loss: 0.14209\n",
            "eval avg reward: 1208.95434\n",
            "eval avg ep len: 406.30000\n",
            "eval d4rl score: 37.76922\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:12:06\n",
            "num of updates: 4400\n",
            "action loss: 0.14202\n",
            "eval avg reward: 1008.51185\n",
            "eval avg ep len: 349.50000\n",
            "eval d4rl score: 31.61043\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:12:27\n",
            "num of updates: 4500\n",
            "action loss: 0.13881\n",
            "eval avg reward: 1198.84348\n",
            "eval avg ep len: 396.50000\n",
            "eval d4rl score: 37.45856\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:12:50\n",
            "num of updates: 4600\n",
            "action loss: 0.13801\n",
            "eval avg reward: 1329.95410\n",
            "eval avg ep len: 435.50000\n",
            "eval d4rl score: 41.48706\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:13:16\n",
            "num of updates: 4700\n",
            "action loss: 0.13639\n",
            "eval avg reward: 1414.40496\n",
            "eval avg ep len: 462.50000\n",
            "eval d4rl score: 44.08190\n",
            "max d4rl score: 46.00099\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:13:42\n",
            "num of updates: 4800\n",
            "action loss: 0.13422\n",
            "eval avg reward: 1567.65628\n",
            "eval avg ep len: 508.70000\n",
            "eval d4rl score: 48.79070\n",
            "max d4rl score: 46.00099\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:14:11\n",
            "num of updates: 4900\n",
            "action loss: 0.13281\n",
            "eval avg reward: 1652.07979\n",
            "eval avg ep len: 530.70000\n",
            "eval d4rl score: 51.38470\n",
            "max d4rl score: 48.79070\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:14:41\n",
            "num of updates: 5000\n",
            "action loss: 0.13132\n",
            "eval avg reward: 1760.64610\n",
            "eval avg ep len: 570.00000\n",
            "eval d4rl score: 54.72051\n",
            "max d4rl score: 51.38470\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:15:11\n",
            "num of updates: 5100\n",
            "action loss: 0.13034\n",
            "eval avg reward: 1819.13735\n",
            "eval avg ep len: 584.90000\n",
            "eval d4rl score: 56.51771\n",
            "max d4rl score: 54.72051\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:15:41\n",
            "num of updates: 5200\n",
            "action loss: 0.13079\n",
            "eval avg reward: 1674.62490\n",
            "eval avg ep len: 537.40000\n",
            "eval d4rl score: 52.07742\n",
            "max d4rl score: 56.51771\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:16:12\n",
            "num of updates: 5300\n",
            "action loss: 0.12806\n",
            "eval avg reward: 1816.67595\n",
            "eval avg ep len: 586.30000\n",
            "eval d4rl score: 56.44208\n",
            "max d4rl score: 56.51771\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:16:43\n",
            "num of updates: 5400\n",
            "action loss: 0.12682\n",
            "eval avg reward: 1806.48700\n",
            "eval avg ep len: 589.00000\n",
            "eval d4rl score: 56.12901\n",
            "max d4rl score: 56.51771\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:17:15\n",
            "num of updates: 5500\n",
            "action loss: 0.12507\n",
            "eval avg reward: 1905.71980\n",
            "eval avg ep len: 625.00000\n",
            "eval d4rl score: 59.17804\n",
            "max d4rl score: 56.51771\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:17:43\n",
            "num of updates: 5600\n",
            "action loss: 0.12488\n",
            "eval avg reward: 1635.08042\n",
            "eval avg ep len: 532.40000\n",
            "eval d4rl score: 50.86237\n",
            "max d4rl score: 59.17804\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:18:16\n",
            "num of updates: 5700\n",
            "action loss: 0.12395\n",
            "eval avg reward: 1872.77137\n",
            "eval avg ep len: 611.40000\n",
            "eval d4rl score: 58.16567\n",
            "max d4rl score: 59.17804\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:18:46\n",
            "num of updates: 5800\n",
            "action loss: 0.12368\n",
            "eval avg reward: 1735.77471\n",
            "eval avg ep len: 558.50000\n",
            "eval d4rl score: 53.95631\n",
            "max d4rl score: 59.17804\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:19:13\n",
            "num of updates: 5900\n",
            "action loss: 0.11995\n",
            "eval avg reward: 1653.38263\n",
            "eval avg ep len: 531.00000\n",
            "eval d4rl score: 51.42473\n",
            "max d4rl score: 59.17804\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:19:47\n",
            "num of updates: 6000\n",
            "action loss: 0.12094\n",
            "eval avg reward: 1943.03477\n",
            "eval avg ep len: 632.20000\n",
            "eval d4rl score: 60.32458\n",
            "max d4rl score: 59.17804\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:20:14\n",
            "num of updates: 6100\n",
            "action loss: 0.11983\n",
            "eval avg reward: 1535.45598\n",
            "eval avg ep len: 497.90000\n",
            "eval d4rl score: 47.80131\n",
            "max d4rl score: 60.32458\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:20:45\n",
            "num of updates: 6200\n",
            "action loss: 0.11850\n",
            "eval avg reward: 1792.00288\n",
            "eval avg ep len: 591.40000\n",
            "eval d4rl score: 55.68397\n",
            "max d4rl score: 60.32458\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:21:14\n",
            "num of updates: 6300\n",
            "action loss: 0.11819\n",
            "eval avg reward: 1764.11948\n",
            "eval avg ep len: 569.90000\n",
            "eval d4rl score: 54.82723\n",
            "max d4rl score: 60.32458\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:21:46\n",
            "num of updates: 6400\n",
            "action loss: 0.11608\n",
            "eval avg reward: 1858.69959\n",
            "eval avg ep len: 598.80000\n",
            "eval d4rl score: 57.73330\n",
            "max d4rl score: 60.32458\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:22:20\n",
            "num of updates: 6500\n",
            "action loss: 0.11561\n",
            "eval avg reward: 2006.07298\n",
            "eval avg ep len: 649.20000\n",
            "eval d4rl score: 62.26149\n",
            "max d4rl score: 60.32458\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:22:51\n",
            "num of updates: 6600\n",
            "action loss: 0.11400\n",
            "eval avg reward: 1748.63537\n",
            "eval avg ep len: 562.30000\n",
            "eval d4rl score: 54.35146\n",
            "max d4rl score: 62.26149\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:23:22\n",
            "num of updates: 6700\n",
            "action loss: 0.11353\n",
            "eval avg reward: 1792.85965\n",
            "eval avg ep len: 589.50000\n",
            "eval d4rl score: 55.71030\n",
            "max d4rl score: 62.26149\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:23:58\n",
            "num of updates: 6800\n",
            "action loss: 0.11279\n",
            "eval avg reward: 2102.71554\n",
            "eval avg ep len: 684.70000\n",
            "eval d4rl score: 65.23093\n",
            "max d4rl score: 62.26149\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:24:29\n",
            "num of updates: 6900\n",
            "action loss: 0.11314\n",
            "eval avg reward: 1840.31213\n",
            "eval avg ep len: 600.80000\n",
            "eval d4rl score: 57.16832\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:24:58\n",
            "num of updates: 7000\n",
            "action loss: 0.11158\n",
            "eval avg reward: 1748.98988\n",
            "eval avg ep len: 562.10000\n",
            "eval d4rl score: 54.36236\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:25:23\n",
            "num of updates: 7100\n",
            "action loss: 0.11080\n",
            "eval avg reward: 1317.75844\n",
            "eval avg ep len: 442.00000\n",
            "eval d4rl score: 41.11234\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:25:52\n",
            "num of updates: 7200\n",
            "action loss: 0.11003\n",
            "eval avg reward: 1811.81898\n",
            "eval avg ep len: 575.70000\n",
            "eval d4rl score: 56.29284\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:26:27\n",
            "num of updates: 7300\n",
            "action loss: 0.10849\n",
            "eval avg reward: 2014.04455\n",
            "eval avg ep len: 654.00000\n",
            "eval d4rl score: 62.50643\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:26:58\n",
            "num of updates: 7400\n",
            "action loss: 0.10779\n",
            "eval avg reward: 1818.14164\n",
            "eval avg ep len: 593.20000\n",
            "eval d4rl score: 56.48711\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:27:30\n",
            "num of updates: 7500\n",
            "action loss: 0.10720\n",
            "eval avg reward: 1840.41224\n",
            "eval avg ep len: 592.10000\n",
            "eval d4rl score: 57.17140\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:28:02\n",
            "num of updates: 7600\n",
            "action loss: 0.10575\n",
            "eval avg reward: 1823.62667\n",
            "eval avg ep len: 593.60000\n",
            "eval d4rl score: 56.65565\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:28:29\n",
            "num of updates: 7700\n",
            "action loss: 0.10427\n",
            "eval avg reward: 1705.92267\n",
            "eval avg ep len: 542.40000\n",
            "eval d4rl score: 53.03907\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:29:02\n",
            "num of updates: 7800\n",
            "action loss: 0.10427\n",
            "eval avg reward: 1943.55635\n",
            "eval avg ep len: 617.70000\n",
            "eval d4rl score: 60.34061\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:29:35\n",
            "num of updates: 7900\n",
            "action loss: 0.10486\n",
            "eval avg reward: 2014.84884\n",
            "eval avg ep len: 634.40000\n",
            "eval d4rl score: 62.53114\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:30:08\n",
            "num of updates: 8000\n",
            "action loss: 0.10308\n",
            "eval avg reward: 1896.53315\n",
            "eval avg ep len: 618.20000\n",
            "eval d4rl score: 58.89577\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:30:37\n",
            "num of updates: 8100\n",
            "action loss: 0.10306\n",
            "eval avg reward: 1658.64950\n",
            "eval avg ep len: 530.70000\n",
            "eval d4rl score: 51.58656\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:31:09\n",
            "num of updates: 8200\n",
            "action loss: 0.10203\n",
            "eval avg reward: 1972.40996\n",
            "eval avg ep len: 621.50000\n",
            "eval d4rl score: 61.22716\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:31:40\n",
            "num of updates: 8300\n",
            "action loss: 0.10092\n",
            "eval avg reward: 1880.24234\n",
            "eval avg ep len: 609.50000\n",
            "eval d4rl score: 58.39522\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:32:14\n",
            "num of updates: 8400\n",
            "action loss: 0.09979\n",
            "eval avg reward: 2062.84227\n",
            "eval avg ep len: 652.00000\n",
            "eval d4rl score: 64.00579\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:32:44\n",
            "num of updates: 8500\n",
            "action loss: 0.09913\n",
            "eval avg reward: 1751.18793\n",
            "eval avg ep len: 558.30000\n",
            "eval d4rl score: 54.42989\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:33:14\n",
            "num of updates: 8600\n",
            "action loss: 0.09996\n",
            "eval avg reward: 1728.65412\n",
            "eval avg ep len: 548.00000\n",
            "eval d4rl score: 53.73752\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:33:45\n",
            "num of updates: 8700\n",
            "action loss: 0.09962\n",
            "eval avg reward: 1885.56669\n",
            "eval avg ep len: 598.70000\n",
            "eval d4rl score: 58.55882\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:34:15\n",
            "num of updates: 8800\n",
            "action loss: 0.09847\n",
            "eval avg reward: 1832.09447\n",
            "eval avg ep len: 591.30000\n",
            "eval d4rl score: 56.91583\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:34:49\n",
            "num of updates: 8900\n",
            "action loss: 0.09746\n",
            "eval avg reward: 2013.04798\n",
            "eval avg ep len: 640.60000\n",
            "eval d4rl score: 62.47581\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:35:23\n",
            "num of updates: 9000\n",
            "action loss: 0.09633\n",
            "eval avg reward: 1988.45769\n",
            "eval avg ep len: 641.80000\n",
            "eval d4rl score: 61.72024\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:35:55\n",
            "num of updates: 9100\n",
            "action loss: 0.09500\n",
            "eval avg reward: 1901.23864\n",
            "eval avg ep len: 606.80000\n",
            "eval d4rl score: 59.04035\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:36:30\n",
            "num of updates: 9200\n",
            "action loss: 0.09514\n",
            "eval avg reward: 2052.65923\n",
            "eval avg ep len: 656.20000\n",
            "eval d4rl score: 63.69290\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:37:03\n",
            "num of updates: 9300\n",
            "action loss: 0.09514\n",
            "eval avg reward: 1927.99996\n",
            "eval avg ep len: 613.50000\n",
            "eval d4rl score: 59.86262\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:37:35\n",
            "num of updates: 9400\n",
            "action loss: 0.09381\n",
            "eval avg reward: 1903.71031\n",
            "eval avg ep len: 609.50000\n",
            "eval d4rl score: 59.11630\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:38:07\n",
            "num of updates: 9500\n",
            "action loss: 0.09306\n",
            "eval avg reward: 1870.73714\n",
            "eval avg ep len: 597.30000\n",
            "eval d4rl score: 58.10316\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:38:36\n",
            "num of updates: 9600\n",
            "action loss: 0.09418\n",
            "eval avg reward: 1783.24976\n",
            "eval avg ep len: 564.90000\n",
            "eval d4rl score: 55.41503\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:39:06\n",
            "num of updates: 9700\n",
            "action loss: 0.09288\n",
            "eval avg reward: 1790.83241\n",
            "eval avg ep len: 571.90000\n",
            "eval d4rl score: 55.64801\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:39:40\n",
            "num of updates: 9800\n",
            "action loss: 0.09223\n",
            "eval avg reward: 1929.31060\n",
            "eval avg ep len: 622.90000\n",
            "eval d4rl score: 59.90289\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:40:13\n",
            "num of updates: 9900\n",
            "action loss: 0.09177\n",
            "eval avg reward: 1954.88359\n",
            "eval avg ep len: 624.00000\n",
            "eval d4rl score: 60.68865\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:40:42\n",
            "num of updates: 10000\n",
            "action loss: 0.09047\n",
            "eval avg reward: 1695.62589\n",
            "eval avg ep len: 541.80000\n",
            "eval d4rl score: 52.72269\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:41:13\n",
            "num of updates: 10100\n",
            "action loss: 0.09149\n",
            "eval avg reward: 1883.28235\n",
            "eval avg ep len: 601.90000\n",
            "eval d4rl score: 58.48863\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:41:43\n",
            "num of updates: 10200\n",
            "action loss: 0.08935\n",
            "eval avg reward: 1783.28529\n",
            "eval avg ep len: 566.10000\n",
            "eval d4rl score: 55.41612\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:42:16\n",
            "num of updates: 10300\n",
            "action loss: 0.08945\n",
            "eval avg reward: 1971.04339\n",
            "eval avg ep len: 618.80000\n",
            "eval d4rl score: 61.18517\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:42:45\n",
            "num of updates: 10400\n",
            "action loss: 0.08864\n",
            "eval avg reward: 1709.23590\n",
            "eval avg ep len: 542.00000\n",
            "eval d4rl score: 53.14088\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:43:14\n",
            "num of updates: 10500\n",
            "action loss: 0.08902\n",
            "eval avg reward: 1781.55011\n",
            "eval avg ep len: 570.50000\n",
            "eval d4rl score: 55.36280\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:43:45\n",
            "num of updates: 10600\n",
            "action loss: 0.08801\n",
            "eval avg reward: 1892.44467\n",
            "eval avg ep len: 599.60000\n",
            "eval d4rl score: 58.77015\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:44:16\n",
            "num of updates: 10700\n",
            "action loss: 0.08762\n",
            "eval avg reward: 1745.81191\n",
            "eval avg ep len: 551.90000\n",
            "eval d4rl score: 54.26471\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:44:47\n",
            "num of updates: 10800\n",
            "action loss: 0.08770\n",
            "eval avg reward: 1863.08807\n",
            "eval avg ep len: 591.40000\n",
            "eval d4rl score: 57.86814\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:45:18\n",
            "num of updates: 10900\n",
            "action loss: 0.08712\n",
            "eval avg reward: 1858.06328\n",
            "eval avg ep len: 584.10000\n",
            "eval d4rl score: 57.71375\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:45:50\n",
            "num of updates: 11000\n",
            "action loss: 0.08665\n",
            "eval avg reward: 1960.67887\n",
            "eval avg ep len: 620.80000\n",
            "eval d4rl score: 60.86671\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:46:20\n",
            "num of updates: 11100\n",
            "action loss: 0.08654\n",
            "eval avg reward: 1786.06978\n",
            "eval avg ep len: 573.20000\n",
            "eval d4rl score: 55.50167\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:46:52\n",
            "num of updates: 11200\n",
            "action loss: 0.08571\n",
            "eval avg reward: 1812.51216\n",
            "eval avg ep len: 580.80000\n",
            "eval d4rl score: 56.31414\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:47:24\n",
            "num of updates: 11300\n",
            "action loss: 0.08648\n",
            "eval avg reward: 1846.25899\n",
            "eval avg ep len: 591.30000\n",
            "eval d4rl score: 57.35105\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:47:57\n",
            "num of updates: 11400\n",
            "action loss: 0.08440\n",
            "eval avg reward: 1985.25982\n",
            "eval avg ep len: 625.70000\n",
            "eval d4rl score: 61.62199\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:48:25\n",
            "num of updates: 11500\n",
            "action loss: 0.08455\n",
            "eval avg reward: 1738.67020\n",
            "eval avg ep len: 549.40000\n",
            "eval d4rl score: 54.04527\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:48:59\n",
            "num of updates: 11600\n",
            "action loss: 0.08422\n",
            "eval avg reward: 2011.74596\n",
            "eval avg ep len: 631.70000\n",
            "eval d4rl score: 62.43580\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:49:28\n",
            "num of updates: 11700\n",
            "action loss: 0.08473\n",
            "eval avg reward: 1651.05192\n",
            "eval avg ep len: 530.90000\n",
            "eval d4rl score: 51.35311\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:49:56\n",
            "num of updates: 11800\n",
            "action loss: 0.08331\n",
            "eval avg reward: 1682.86684\n",
            "eval avg ep len: 533.50000\n",
            "eval d4rl score: 52.33066\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:50:26\n",
            "num of updates: 11900\n",
            "action loss: 0.08361\n",
            "eval avg reward: 1800.31709\n",
            "eval avg ep len: 567.50000\n",
            "eval d4rl score: 55.93944\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:50:59\n",
            "num of updates: 12000\n",
            "action loss: 0.08214\n",
            "eval avg reward: 1900.54988\n",
            "eval avg ep len: 617.80000\n",
            "eval d4rl score: 59.01919\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:51:30\n",
            "num of updates: 12100\n",
            "action loss: 0.08255\n",
            "eval avg reward: 1850.62880\n",
            "eval avg ep len: 586.00000\n",
            "eval d4rl score: 57.48531\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:52:05\n",
            "num of updates: 12200\n",
            "action loss: 0.08249\n",
            "eval avg reward: 2034.04802\n",
            "eval avg ep len: 651.60000\n",
            "eval d4rl score: 63.12105\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:52:35\n",
            "num of updates: 12300\n",
            "action loss: 0.08273\n",
            "eval avg reward: 1696.08195\n",
            "eval avg ep len: 543.70000\n",
            "eval d4rl score: 52.73671\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:53:03\n",
            "num of updates: 12400\n",
            "action loss: 0.08185\n",
            "eval avg reward: 1698.66322\n",
            "eval avg ep len: 543.80000\n",
            "eval d4rl score: 52.81602\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:53:34\n",
            "num of updates: 12500\n",
            "action loss: 0.08126\n",
            "eval avg reward: 1834.88029\n",
            "eval avg ep len: 583.70000\n",
            "eval d4rl score: 57.00143\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:54:09\n",
            "num of updates: 12600\n",
            "action loss: 0.08073\n",
            "eval avg reward: 2032.50981\n",
            "eval avg ep len: 654.90000\n",
            "eval d4rl score: 63.07379\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:54:41\n",
            "num of updates: 12700\n",
            "action loss: 0.08117\n",
            "eval avg reward: 1887.07249\n",
            "eval avg ep len: 589.70000\n",
            "eval d4rl score: 58.60508\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:55:14\n",
            "num of updates: 12800\n",
            "action loss: 0.08127\n",
            "eval avg reward: 1906.23677\n",
            "eval avg ep len: 609.50000\n",
            "eval d4rl score: 59.19392\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:55:41\n",
            "num of updates: 12900\n",
            "action loss: 0.08019\n",
            "eval avg reward: 1686.50723\n",
            "eval avg ep len: 537.20000\n",
            "eval d4rl score: 52.44251\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:56:14\n",
            "num of updates: 13000\n",
            "action loss: 0.07964\n",
            "eval avg reward: 1959.17637\n",
            "eval avg ep len: 615.70000\n",
            "eval d4rl score: 60.82055\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:56:47\n",
            "num of updates: 13100\n",
            "action loss: 0.07984\n",
            "eval avg reward: 1900.32100\n",
            "eval avg ep len: 595.30000\n",
            "eval d4rl score: 59.01216\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:57:15\n",
            "num of updates: 13200\n",
            "action loss: 0.08033\n",
            "eval avg reward: 1602.78125\n",
            "eval avg ep len: 511.30000\n",
            "eval d4rl score: 49.86995\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:57:41\n",
            "num of updates: 13300\n",
            "action loss: 0.07962\n",
            "eval avg reward: 1644.89463\n",
            "eval avg ep len: 516.70000\n",
            "eval d4rl score: 51.16393\n",
            "max d4rl score: 65.23093\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:58:17\n",
            "num of updates: 13400\n",
            "action loss: 0.07852\n",
            "eval avg reward: 2173.28314\n",
            "eval avg ep len: 685.10000\n",
            "eval d4rl score: 67.39919\n",
            "max d4rl score: 65.23093\n",
            "saving max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:58:49\n",
            "num of updates: 13500\n",
            "action loss: 0.07852\n",
            "eval avg reward: 1869.21128\n",
            "eval avg ep len: 597.30000\n",
            "eval d4rl score: 58.05628\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:59:23\n",
            "num of updates: 13600\n",
            "action loss: 0.07873\n",
            "eval avg reward: 1999.53658\n",
            "eval avg ep len: 641.40000\n",
            "eval d4rl score: 62.06065\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 0:59:52\n",
            "num of updates: 13700\n",
            "action loss: 0.07823\n",
            "eval avg reward: 1666.73803\n",
            "eval avg ep len: 533.00000\n",
            "eval d4rl score: 51.83509\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:00:22\n",
            "num of updates: 13800\n",
            "action loss: 0.07785\n",
            "eval avg reward: 1800.60891\n",
            "eval avg ep len: 568.50000\n",
            "eval d4rl score: 55.94840\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:00:52\n",
            "num of updates: 13900\n",
            "action loss: 0.07808\n",
            "eval avg reward: 1756.41125\n",
            "eval avg ep len: 553.00000\n",
            "eval d4rl score: 54.59039\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:01:07\n",
            "num of updates: 14000\n",
            "action loss: 0.07681\n",
            "eval avg reward: 678.17392\n",
            "eval avg ep len: 251.30000\n",
            "eval d4rl score: 21.46046\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:01:39\n",
            "num of updates: 14100\n",
            "action loss: 0.07672\n",
            "eval avg reward: 1888.76603\n",
            "eval avg ep len: 596.50000\n",
            "eval d4rl score: 58.65712\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:02:11\n",
            "num of updates: 14200\n",
            "action loss: 0.07656\n",
            "eval avg reward: 1817.26985\n",
            "eval avg ep len: 578.30000\n",
            "eval d4rl score: 56.46033\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:02:41\n",
            "num of updates: 14300\n",
            "action loss: 0.07591\n",
            "eval avg reward: 1801.07899\n",
            "eval avg ep len: 578.10000\n",
            "eval d4rl score: 55.96285\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:03:11\n",
            "num of updates: 14400\n",
            "action loss: 0.07638\n",
            "eval avg reward: 1732.56677\n",
            "eval avg ep len: 549.30000\n",
            "eval d4rl score: 53.85774\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:03:41\n",
            "num of updates: 14500\n",
            "action loss: 0.07605\n",
            "eval avg reward: 1666.37915\n",
            "eval avg ep len: 529.90000\n",
            "eval d4rl score: 51.82406\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:04:06\n",
            "num of updates: 14600\n",
            "action loss: 0.07610\n",
            "eval avg reward: 1536.97404\n",
            "eval avg ep len: 484.90000\n",
            "eval d4rl score: 47.84796\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:04:38\n",
            "num of updates: 14700\n",
            "action loss: 0.07546\n",
            "eval avg reward: 1800.24985\n",
            "eval avg ep len: 571.40000\n",
            "eval d4rl score: 55.93737\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:05:10\n",
            "num of updates: 14800\n",
            "action loss: 0.07545\n",
            "eval avg reward: 1852.05388\n",
            "eval avg ep len: 594.00000\n",
            "eval d4rl score: 57.52910\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:05:41\n",
            "num of updates: 14900\n",
            "action loss: 0.07593\n",
            "eval avg reward: 1750.41656\n",
            "eval avg ep len: 556.60000\n",
            "eval d4rl score: 54.40619\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:06:11\n",
            "num of updates: 15000\n",
            "action loss: 0.07465\n",
            "eval avg reward: 1831.40957\n",
            "eval avg ep len: 578.10000\n",
            "eval d4rl score: 56.89478\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:06:41\n",
            "num of updates: 15100\n",
            "action loss: 0.07389\n",
            "eval avg reward: 1707.92069\n",
            "eval avg ep len: 539.30000\n",
            "eval d4rl score: 53.10046\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:07:12\n",
            "num of updates: 15200\n",
            "action loss: 0.07523\n",
            "eval avg reward: 1822.12201\n",
            "eval avg ep len: 572.00000\n",
            "eval d4rl score: 56.60941\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:07:37\n",
            "num of updates: 15300\n",
            "action loss: 0.07442\n",
            "eval avg reward: 1463.09397\n",
            "eval avg ep len: 471.80000\n",
            "eval d4rl score: 45.57792\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:08:07\n",
            "num of updates: 15400\n",
            "action loss: 0.07509\n",
            "eval avg reward: 1692.48082\n",
            "eval avg ep len: 540.10000\n",
            "eval d4rl score: 52.62606\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:08:38\n",
            "num of updates: 15500\n",
            "action loss: 0.07400\n",
            "eval avg reward: 1757.29053\n",
            "eval avg ep len: 560.60000\n",
            "eval d4rl score: 54.61740\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:09:08\n",
            "num of updates: 15600\n",
            "action loss: 0.07481\n",
            "eval avg reward: 1840.57643\n",
            "eval avg ep len: 585.70000\n",
            "eval d4rl score: 57.17644\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:09:41\n",
            "num of updates: 15700\n",
            "action loss: 0.07399\n",
            "eval avg reward: 1953.28494\n",
            "eval avg ep len: 614.80000\n",
            "eval d4rl score: 60.63953\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:10:09\n",
            "num of updates: 15800\n",
            "action loss: 0.07349\n",
            "eval avg reward: 1648.24810\n",
            "eval avg ep len: 523.00000\n",
            "eval d4rl score: 51.26696\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:10:38\n",
            "num of updates: 15900\n",
            "action loss: 0.07321\n",
            "eval avg reward: 1600.58009\n",
            "eval avg ep len: 512.60000\n",
            "eval d4rl score: 49.80232\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:11:07\n",
            "num of updates: 16000\n",
            "action loss: 0.07319\n",
            "eval avg reward: 1770.67039\n",
            "eval avg ep len: 562.70000\n",
            "eval d4rl score: 55.02851\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:11:37\n",
            "num of updates: 16100\n",
            "action loss: 0.07374\n",
            "eval avg reward: 1716.85122\n",
            "eval avg ep len: 549.20000\n",
            "eval d4rl score: 53.37486\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:12:08\n",
            "num of updates: 16200\n",
            "action loss: 0.07318\n",
            "eval avg reward: 1760.20394\n",
            "eval avg ep len: 559.60000\n",
            "eval d4rl score: 54.70692\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:12:36\n",
            "num of updates: 16300\n",
            "action loss: 0.07334\n",
            "eval avg reward: 1690.01498\n",
            "eval avg ep len: 538.80000\n",
            "eval d4rl score: 52.55029\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:13:06\n",
            "num of updates: 16400\n",
            "action loss: 0.07221\n",
            "eval avg reward: 1759.37804\n",
            "eval avg ep len: 555.40000\n",
            "eval d4rl score: 54.68154\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:13:35\n",
            "num of updates: 16500\n",
            "action loss: 0.07313\n",
            "eval avg reward: 1693.21342\n",
            "eval avg ep len: 529.50000\n",
            "eval d4rl score: 52.64857\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:14:03\n",
            "num of updates: 16600\n",
            "action loss: 0.07252\n",
            "eval avg reward: 1703.01975\n",
            "eval avg ep len: 537.00000\n",
            "eval d4rl score: 52.94988\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:14:35\n",
            "num of updates: 16700\n",
            "action loss: 0.07257\n",
            "eval avg reward: 1942.68651\n",
            "eval avg ep len: 622.00000\n",
            "eval d4rl score: 60.31388\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:15:05\n",
            "num of updates: 16800\n",
            "action loss: 0.07162\n",
            "eval avg reward: 1710.95862\n",
            "eval avg ep len: 539.00000\n",
            "eval d4rl score: 53.19381\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:15:34\n",
            "num of updates: 16900\n",
            "action loss: 0.07204\n",
            "eval avg reward: 1729.82962\n",
            "eval avg ep len: 549.30000\n",
            "eval d4rl score: 53.77364\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:16:04\n",
            "num of updates: 17000\n",
            "action loss: 0.07152\n",
            "eval avg reward: 1843.43672\n",
            "eval avg ep len: 582.90000\n",
            "eval d4rl score: 57.26433\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:16:37\n",
            "num of updates: 17100\n",
            "action loss: 0.07136\n",
            "eval avg reward: 1985.12452\n",
            "eval avg ep len: 627.00000\n",
            "eval d4rl score: 61.61783\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:17:09\n",
            "num of updates: 17200\n",
            "action loss: 0.07146\n",
            "eval avg reward: 1861.26218\n",
            "eval avg ep len: 589.10000\n",
            "eval d4rl score: 57.81204\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:17:37\n",
            "num of updates: 17300\n",
            "action loss: 0.07138\n",
            "eval avg reward: 1588.40238\n",
            "eval avg ep len: 498.40000\n",
            "eval d4rl score: 49.42814\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:18:07\n",
            "num of updates: 17400\n",
            "action loss: 0.07140\n",
            "eval avg reward: 1855.34598\n",
            "eval avg ep len: 587.60000\n",
            "eval d4rl score: 57.63025\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:18:40\n",
            "num of updates: 17500\n",
            "action loss: 0.07086\n",
            "eval avg reward: 2058.90499\n",
            "eval avg ep len: 645.30000\n",
            "eval d4rl score: 63.88481\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:19:00\n",
            "num of updates: 17600\n",
            "action loss: 0.07085\n",
            "eval avg reward: 1047.74549\n",
            "eval avg ep len: 348.50000\n",
            "eval d4rl score: 32.81592\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:19:29\n",
            "num of updates: 17700\n",
            "action loss: 0.07085\n",
            "eval avg reward: 1737.38834\n",
            "eval avg ep len: 555.10000\n",
            "eval d4rl score: 54.00589\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:19:58\n",
            "num of updates: 17800\n",
            "action loss: 0.07096\n",
            "eval avg reward: 1720.78721\n",
            "eval avg ep len: 544.50000\n",
            "eval d4rl score: 53.49580\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:20:27\n",
            "num of updates: 17900\n",
            "action loss: 0.07044\n",
            "eval avg reward: 1716.89985\n",
            "eval avg ep len: 550.00000\n",
            "eval d4rl score: 53.37636\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:20:56\n",
            "num of updates: 18000\n",
            "action loss: 0.07054\n",
            "eval avg reward: 1742.95531\n",
            "eval avg ep len: 551.50000\n",
            "eval d4rl score: 54.17694\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:21:28\n",
            "num of updates: 18100\n",
            "action loss: 0.07024\n",
            "eval avg reward: 1875.10833\n",
            "eval avg ep len: 592.80000\n",
            "eval d4rl score: 58.23747\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:21:57\n",
            "num of updates: 18200\n",
            "action loss: 0.07046\n",
            "eval avg reward: 1680.58511\n",
            "eval avg ep len: 527.50000\n",
            "eval d4rl score: 52.26055\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:22:24\n",
            "num of updates: 18300\n",
            "action loss: 0.07016\n",
            "eval avg reward: 1640.67219\n",
            "eval avg ep len: 523.40000\n",
            "eval d4rl score: 51.03419\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:22:55\n",
            "num of updates: 18400\n",
            "action loss: 0.07083\n",
            "eval avg reward: 1780.10742\n",
            "eval avg ep len: 562.80000\n",
            "eval d4rl score: 55.31847\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:23:24\n",
            "num of updates: 18500\n",
            "action loss: 0.06976\n",
            "eval avg reward: 1714.48576\n",
            "eval avg ep len: 539.30000\n",
            "eval d4rl score: 53.30218\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:23:54\n",
            "num of updates: 18600\n",
            "action loss: 0.06920\n",
            "eval avg reward: 1836.88005\n",
            "eval avg ep len: 579.40000\n",
            "eval d4rl score: 57.06287\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:24:22\n",
            "num of updates: 18700\n",
            "action loss: 0.06915\n",
            "eval avg reward: 1679.76468\n",
            "eval avg ep len: 530.00000\n",
            "eval d4rl score: 52.23534\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:24:49\n",
            "num of updates: 18800\n",
            "action loss: 0.06984\n",
            "eval avg reward: 1611.17409\n",
            "eval avg ep len: 505.80000\n",
            "eval d4rl score: 50.12783\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:25:16\n",
            "num of updates: 18900\n",
            "action loss: 0.06976\n",
            "eval avg reward: 1694.69619\n",
            "eval avg ep len: 537.00000\n",
            "eval d4rl score: 52.69413\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:25:44\n",
            "num of updates: 19000\n",
            "action loss: 0.07041\n",
            "eval avg reward: 1611.14408\n",
            "eval avg ep len: 508.20000\n",
            "eval d4rl score: 50.12691\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:26:13\n",
            "num of updates: 19100\n",
            "action loss: 0.06920\n",
            "eval avg reward: 1818.41250\n",
            "eval avg ep len: 566.70000\n",
            "eval d4rl score: 56.49544\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:26:41\n",
            "num of updates: 19200\n",
            "action loss: 0.06952\n",
            "eval avg reward: 1652.88127\n",
            "eval avg ep len: 527.20000\n",
            "eval d4rl score: 51.40932\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:27:11\n",
            "num of updates: 19300\n",
            "action loss: 0.06857\n",
            "eval avg reward: 1760.79078\n",
            "eval avg ep len: 547.80000\n",
            "eval d4rl score: 54.72495\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:27:40\n",
            "num of updates: 19400\n",
            "action loss: 0.06923\n",
            "eval avg reward: 1789.00987\n",
            "eval avg ep len: 564.40000\n",
            "eval d4rl score: 55.59201\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:28:05\n",
            "num of updates: 19500\n",
            "action loss: 0.06886\n",
            "eval avg reward: 1509.56431\n",
            "eval avg ep len: 480.90000\n",
            "eval d4rl score: 47.00577\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:28:34\n",
            "num of updates: 19600\n",
            "action loss: 0.06867\n",
            "eval avg reward: 1678.40107\n",
            "eval avg ep len: 530.00000\n",
            "eval d4rl score: 52.19344\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:29:02\n",
            "num of updates: 19700\n",
            "action loss: 0.06867\n",
            "eval avg reward: 1768.42271\n",
            "eval avg ep len: 557.10000\n",
            "eval d4rl score: 54.95945\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:29:33\n",
            "num of updates: 19800\n",
            "action loss: 0.06852\n",
            "eval avg reward: 1793.31581\n",
            "eval avg ep len: 567.00000\n",
            "eval d4rl score: 55.72431\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:30:04\n",
            "num of updates: 19900\n",
            "action loss: 0.06883\n",
            "eval avg reward: 1762.78231\n",
            "eval avg ep len: 559.30000\n",
            "eval d4rl score: 54.78614\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "time elapsed: 1:30:34\n",
            "num of updates: 20000\n",
            "action loss: 0.06842\n",
            "eval avg reward: 1788.78072\n",
            "eval avg ep len: 567.20000\n",
            "eval d4rl score: 55.58497\n",
            "max d4rl score: 67.39919\n",
            "saving current model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n",
            "finished training!\n",
            "============================================================\n",
            "started training at: 23-07-05-12-16-25\n",
            "finished training at: 23-07-05-13-46-59\n",
            "total training time: 1:30:34\n",
            "max d4rl score: 67.39919\n",
            "saved max d4rl score model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "saved last updated model at: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25.pt\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-8c6b813766d1>\u001b[0m in \u001b[0;36m<cell line: 169>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m \u001b[0mcsv_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_csv.writer' object has no attribute 'close'"
          ]
        }
      ],
      "source": [
        "\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "\n",
        "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "prefix = \"dt_\" + env_d4rl_name\n",
        "\n",
        "save_model_name =  prefix + \"_lstmmodel_\" + start_time_str + \".pt\"\n",
        "save_model_path = os.path.join(log_dir, save_model_name)\n",
        "save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n",
        "\n",
        "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
        "log_csv_path = os.path.join(log_dir, log_csv_name)\n",
        "\n",
        "\n",
        "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
        "csv_header = ([\"duration\", \"num_updates\", \"action_loss\",\n",
        "               \"eval_avg_reward\", \"eval_avg_ep_len\", \"eval_d4rl_score\"])\n",
        "\n",
        "csv_writer.writerow(csv_header)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"start time: \" + start_time_str)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"device set to: \" + str(device))\n",
        "print(\"dataset path: \" + dataset_path)\n",
        "print(\"model save path: \" + save_model_path)\n",
        "print(\"log csv save path: \" + log_csv_path)\n",
        "\n",
        "\n",
        "traj_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n",
        "\n",
        "traj_data_loader = DataLoader(traj_dataset,\n",
        "\t\t\t\t\t\tbatch_size=batch_size,\n",
        "\t\t\t\t\t\tshuffle=True,\n",
        "\t\t\t\t\t\tpin_memory=True,\n",
        "\t\t\t\t\t\tdrop_last=True)\n",
        "\n",
        "data_iter = iter(traj_data_loader)\n",
        "\n",
        "## get state stats from dataset\n",
        "state_mean, state_std = traj_dataset.get_state_stats()\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "\n",
        "model = DecisionLstm(\n",
        "\t\t\tstate_dim=state_dim,\n",
        "\t\t\tact_dim=act_dim,\n",
        "\t\t\tn_blocks=n_blocks,\n",
        "\t\t\th_dim=embed_dim,\n",
        "\t\t\tcontext_len=context_len,\n",
        "\t\t\tdrop_p=dropout_p,\n",
        "\t\t).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "\t\t\t\t\tmodel.parameters(),\n",
        "\t\t\t\t\tlr=lr,\n",
        "\t\t\t\t\tweight_decay=wt_decay\n",
        "\t\t\t\t)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "\t\toptimizer,\n",
        "\t\tlambda steps: min((steps+1)/warmup_steps, 1)\n",
        "\t)\n",
        "\n",
        "max_d4rl_score = -1.0\n",
        "total_updates = 0\n",
        "\n",
        "for i_train_iter in range(max_train_iters):\n",
        "\n",
        "\tlog_action_losses = []\n",
        "\tmodel.train()\n",
        "\n",
        "\tfor _ in range(num_updates_per_iter):\n",
        "\t\ttry:\n",
        "\t\t\ttimesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "\t\texcept StopIteration:\n",
        "\t\t\tdata_iter = iter(traj_data_loader)\n",
        "\t\t\ttimesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "\n",
        "\t\ttimesteps = timesteps.to(device)\t# B x T\n",
        "\t\tstates = states.to(device)\t\t\t# B x T x state_dim\n",
        "\t\tactions = actions.to(device)\t\t# B x T x act_dim\n",
        "\t\treturns_to_go = returns_to_go.to(device).unsqueeze(dim=-1) # B x T x 1\n",
        "\t\ttraj_mask = traj_mask.to(device)\t# B x T\n",
        "\n",
        "\t\taction_target = torch.clone(actions).detach().to(device)\n",
        "\n",
        "\t\tstate_preds, action_preds, return_preds = model.forward(\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\treturns_to_go=returns_to_go\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
        "\n",
        "\t\t# only consider non padded elements\n",
        "\t\taction_preds = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
        "\t\taction_target = action_target.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
        "\n",
        "\t\taction_loss = F.mse_loss(action_preds, action_target, reduction='mean')\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\taction_loss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "\t\toptimizer.step()\n",
        "\t\tscheduler.step()\n",
        "\n",
        "\t\tlog_action_losses.append(action_loss.detach().cpu().item())\n",
        "\n",
        "\t# evaluate on env\n",
        "\tresults = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
        "\t                        num_eval_ep, max_eval_ep_len, state_mean, state_std,\n",
        "\t\t\t\t\t\t\t)\n",
        "\teval_avg_reward = results['eval/avg_reward']\n",
        "\teval_avg_ep_len = results['eval/avg_ep_len']\n",
        "\teval_d4rl_score = get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
        "\n",
        "\tmean_action_loss = np.mean(log_action_losses)\n",
        "\ttime_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
        "\n",
        "\ttotal_updates += num_updates_per_iter\n",
        "\n",
        "\tlog_str = (\"=\" * 60 + '\\n' +\n",
        "\t\t\t\"time elapsed: \" + time_elapsed  + '\\n' +\n",
        "\t\t\t\"num of updates: \" + str(total_updates) + '\\n' +\n",
        "\t\t\t\"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n",
        "\t\t\t\"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n",
        "\t\t\t\"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
        "\t\t\t\"eval d4rl score: \" + format(eval_d4rl_score, \".5f\")\n",
        "\t\t\t)\n",
        "\n",
        "\tprint(log_str)\n",
        "\n",
        "\tlog_data = [time_elapsed, total_updates, mean_action_loss,\n",
        "\t\t\t\teval_avg_reward, eval_avg_ep_len,\n",
        "\t\t\t\teval_d4rl_score]\n",
        "\n",
        "\tcsv_writer.writerow(log_data)\n",
        "\n",
        "\t# save model\n",
        "\tprint(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
        "\tif eval_d4rl_score >= max_d4rl_score:\n",
        "\t\tprint(\"saving max d4rl score model at: \" + save_best_model_path)\n",
        "\t\ttorch.save(model.state_dict(), save_best_model_path)\n",
        "\t\tmax_d4rl_score = eval_d4rl_score\n",
        "\n",
        "\tprint(\"saving current model at: \" + save_model_path)\n",
        "\ttorch.save(model.state_dict(), save_model_path)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"finished training!\")\n",
        "print(\"=\" * 60)\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "time_elapsed = str(end_time - start_time)\n",
        "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "print(\"started training at: \" + start_time_str)\n",
        "print(\"finished training at: \" + end_time_str)\n",
        "print(\"total training time: \" + time_elapsed)\n",
        "print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
        "print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
        "print(\"saved last updated model at: \" + save_model_path)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "csv_writer.close()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prum4oAGlb5P"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eosqWqRRJLsZ"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4-WPlC1VR3q",
        "outputId": "f1182a77-ba3c-463b-d0e9-13e09c270e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
            "model loaded from: ./dt_runs/dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\n",
            "{'eval/avg_reward': 1922.4958449093106, 'eval/avg_ep_len': 611.7}\n",
            "normalized d4rl score:  59.69350095324769\n",
            "============================================================\n",
            "evaluated on env: Hopper-v3\n",
            "total num of checkpoints evaluated: 1\n",
            "d4rl score mean: 59.69350\n",
            "d4rl score std: 0.00000\n",
            "d4rl score var: 0.00000\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# set mujoco env path if not already set\n",
        "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n",
        "\n",
        "\n",
        "eval_dataset = \"medium\"\t\t# medium / medium-replay / medium-expert\n",
        "eval_rtg_scale = 1000\t\t# normalize returns to go\n",
        "\n",
        "# eval_env_name = \"Walker2d-v3\"\n",
        "# eval_rtg_target = 5000\n",
        "# eval_env_d4rl_name = f'walker2d-{eval_dataset}-v2'\n",
        "\n",
        "# eval_env_name = \"HalfCheetah-v3\"\n",
        "# eval_rtg_target = 6000\n",
        "# eval_env_d4rl_name = f'halfcheetah-{eval_dataset}-v2'\n",
        "\n",
        "eval_env_name = \"Hopper-v3\"\n",
        "eval_rtg_target = 3600\n",
        "eval_env_d4rl_name = f'hopper-{eval_dataset}-v2'\n",
        "\n",
        "\n",
        "num_test_eval_ep = 10\t\t\t# num of evaluation episodes\n",
        "eval_max_eval_ep_len = 1000\t\t# max len of one episode\n",
        "\n",
        "\n",
        "context_len = 20        # K in decision transformer\n",
        "n_blocks = 3            # num of transformer blocks\n",
        "embed_dim = 128         # embedding (hidden) dim of transformer\n",
        "n_heads = 1             # num of transformer heads\n",
        "dropout_p = 0.1         # dropout probability\n",
        "\n",
        "\n",
        "eval_chk_pt_dir = \"./dt_runs/\"\n",
        "\n",
        "\n",
        "eval_chk_pt_name = \"dt_hopper-medium-v2_lstmmodel_23-07-05-12-16-25_best.pt\"\n",
        "##\"dt_walker2d-medium-v2_model_22-02-22-09-24-12_best.pt\"\n",
        "eval_chk_pt_list = [eval_chk_pt_name]\n",
        "\n",
        "\n",
        "## manually override check point list\n",
        "## passing a list will evaluate on all checkpoints\n",
        "## and output mean and std score\n",
        "\n",
        "\n",
        "# eval_chk_pt_list = [\n",
        "# \t\"dt_walker2d-medium-v2_model_22-02-20-06-27-12_best.pt\",\n",
        "# \t\"dt_walker2d-medium-v2_model_22-02-20-09-11-30_best.pt\",\n",
        "# \t\"dt_walker2d-medium-v2_model_22-02-22-09-24-12_best.pt\"\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "env_data_stats = get_d4rl_dataset_stats(eval_env_d4rl_name)\n",
        "eval_state_mean = np.array(env_data_stats['state_mean'])\n",
        "eval_state_std = np.array(env_data_stats['state_std'])\n",
        "\n",
        "eval_env = gym.make(eval_env_name)\n",
        "\n",
        "state_dim = eval_env.observation_space.shape[0]\n",
        "act_dim = eval_env.action_space.shape[0]\n",
        "\n",
        "all_scores = []\n",
        "\n",
        "for eval_chk_pt_name in eval_chk_pt_list:\n",
        "\n",
        "\teval_model = DecisionLstm(\n",
        "\t\t\t\tstate_dim=state_dim,\n",
        "\t\t\t\tact_dim=act_dim,\n",
        "\t\t\t\tn_blocks=n_blocks,\n",
        "\t\t\t\th_dim=embed_dim,\n",
        "\t\t\t\tcontext_len=context_len,\n",
        "\t\t\t\tdrop_p=dropout_p,\n",
        "\t\t\t).to(device)\n",
        "\n",
        "\n",
        "\teval_chk_pt_path = os.path.join(eval_chk_pt_dir, eval_chk_pt_name)\n",
        "\n",
        "\t# load checkpoint\n",
        "\teval_model.load_state_dict(torch.load(eval_chk_pt_path, map_location=device))\n",
        "\n",
        "\tprint(\"model loaded from: \" + eval_chk_pt_path)\n",
        "\n",
        "\t# evaluate on env\n",
        "\tresults = evaluate_on_env(eval_model, device, context_len,\n",
        "\t\t\t\t\t\t\teval_env, eval_rtg_target, eval_rtg_scale,\n",
        "\t\t\t\t\t\t\tnum_test_eval_ep, eval_max_eval_ep_len,\n",
        "\t\t\t\t\t\t\teval_state_mean, eval_state_std)\n",
        "\tprint(results)\n",
        "\n",
        "\tnorm_score = get_d4rl_normalized_score(results['eval/avg_reward'], eval_env_name) * 100\n",
        "\tprint(\"normalized d4rl score: \", norm_score)\n",
        "\n",
        "\tall_scores.append(norm_score)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "all_scores = np.array(all_scores)\n",
        "print(\"evaluated on env: \" + eval_env_name)\n",
        "print(\"total num of checkpoints evaluated: \" + str(len(eval_chk_pt_list)))\n",
        "print(\"d4rl score mean: \" + format(all_scores.mean(), \".5f\"))\n",
        "print(\"d4rl score std: \" + format(all_scores.std(), \".5f\"))\n",
        "print(\"d4rl score var: \" + format(all_scores.var(), \".5f\"))\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaacQSsJJKVx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxcJqnb1Him4"
      },
      "source": [
        "## render env\n",
        "\n",
        "\n",
        "\n",
        "*   saves mp4 video of env frames and plays it in notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdf_bea2hiRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3f9e10-4eef-41c9-d98f-7b20f4c31f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender) (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.27.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (1.22.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.4)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3114 sha256=a339748c4f502194cf245122eef74920bb31feff7ce35f43d3572ec43787fd57\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/62/63/7b3acfb684dd3d665d7fc1d213427b136205a222389767e295\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-RpNwa0hiPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "4986fd90-bf44-44de-d0b9-73ded3a4272b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval/avg_reward': 2052.531631989413, 'eval/avg_ep_len': 644.0}\n",
            "normalized d4rl score:  63.68898100082041\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-0ea703ca1664>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"normalized d4rl score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colabgymrender/recorder.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'temp-{start}.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: VideoClip.write_videofile() got an unexpected keyword argument 'progress_bar'"
          ]
        }
      ],
      "source": [
        "\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "num_test_eval_ep = 1\n",
        "eval_max_ep_len = 1000\n",
        "\n",
        "\n",
        "directory = \"./render_video\"\n",
        "eval_env = Recorder(eval_env, directory)\n",
        "\n",
        "results = evaluate_on_env(eval_model, device, context_len,\n",
        "                        eval_env, eval_rtg_target, eval_rtg_scale,\n",
        "                        num_test_eval_ep, eval_max_ep_len,\n",
        "\t\t\t\t\t\teval_state_mean, eval_state_std)\n",
        "print(results)\n",
        "\n",
        "norm_score = get_d4rl_normalized_score(results['eval/avg_reward'], eval_env_name) * 100\n",
        "print(\"normalized d4rl score: \", norm_score)\n",
        "\n",
        "eval_env.play()\n",
        "\n",
        "eval_env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZNV_H78kRSL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjBsdz9mKbZg"
      },
      "source": [
        "# plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "2WM69ti2KaRN",
        "outputId": "734d84aa-5c03-41f5-b09f-54aa25f7813d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:792: MatplotlibDeprecationWarning: The legendHandles attribute was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use legend_handles instead.\n",
            "  handles = leg.legendHandles\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:792: MatplotlibDeprecationWarning: The legendHandles attribute was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use legend_handles instead.\n",
            "  handles = leg.legendHandles\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:792: MatplotlibDeprecationWarning: The legendHandles attribute was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use legend_handles instead.\n",
            "  handles = leg.legendHandles\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:792: MatplotlibDeprecationWarning: The legendHandles attribute was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use legend_handles instead.\n",
            "  handles = leg.legendHandles\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/pandas/plotting/_matplotlib/core.py:792: MatplotlibDeprecationWarning: The legendHandles attribute was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use legend_handles instead.\n",
            "  handles = leg.legendHandles\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt_runs/dt_hopper-medium-v2_log_23-07-05-12-15-25.csv (0, 6)\n",
            "dt_runs/dt_hopper-medium-v2_log_23-07-05-12-16-25.csv (200, 6)\n",
            "dt_runs/dt_hopper-medium-v2_log_23-07-05-12-12-48.csv (0, 6)\n",
            "dt_runs/dt_hopper-medium-v2_log_23-07-05-12-13-53.csv (0, 6)\n",
            "dt_runs/dt_hopper-medium-v2_log_23-07-05-12-00-20.csv (0, 6)\n",
            "dt_runs/dt_hopper-medium-v2_log_23-07-05-12-10-26.csv (0, 6)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#env_d4rl_name = 'walker2d-medium-v2'\n",
        "env_d4rl_name = 'hopper-medium-v2'\n",
        "\n",
        "log_dir = 'dt_runs/'\n",
        "\n",
        "x_key = \"num_updates\"\n",
        "y_key = \"eval_d4rl_score\"\n",
        "y_smoothing_win = 5\n",
        "plot_avg = False\n",
        "save_fig = False\n",
        "\n",
        "if plot_avg:\n",
        "    save_fig_path = env_d4rl_name + \"_avg.png\"\n",
        "else:\n",
        "    save_fig_path = env_d4rl_name + \".png\"\n",
        "\n",
        "\n",
        "all_files = glob.glob(log_dir + f'/dt_{env_d4rl_name}*.csv')\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set_title(env_d4rl_name)\n",
        "\n",
        "if plot_avg:\n",
        "    name_list = []\n",
        "    df_list = []\n",
        "    for filename in all_files:\n",
        "        frame = pd.read_csv(filename, index_col=None, header=0)\n",
        "        print(filename, frame.shape)\n",
        "        frame['y_smooth'] = frame[y_key].rolling(window=y_smoothing_win).mean()\n",
        "        df_list.append(frame)\n",
        "\n",
        "\n",
        "    df_concat = pd.concat(df_list)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    data_avg.plot(x=x_key, y='y_smooth', ax=ax)\n",
        "\n",
        "    ax.set_xlabel(x_key)\n",
        "    ax.set_ylabel(y_key)\n",
        "    ax.legend(['avg of all runs'], loc='lower right')\n",
        "\n",
        "    if save_fig:\n",
        "        plt.savefig(save_fig_path)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "else:\n",
        "    name_list = []\n",
        "    for filename in all_files:\n",
        "        frame = pd.read_csv(filename, index_col=None, header=0)\n",
        "        print(filename, frame.shape)\n",
        "        frame['y_smooth'] = frame[y_key].rolling(window=y_smoothing_win).mean()\n",
        "        frame.plot(x=x_key, y='y_smooth', ax=ax)\n",
        "        name_list.append(filename.split('/')[-1])\n",
        "\n",
        "    ax.set_xlabel(x_key)\n",
        "    ax.set_ylabel(y_key)\n",
        "    ax.legend(name_list, loc='lower right')\n",
        "\n",
        "    if save_fig:\n",
        "        plt.savefig(save_fig_path)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvUFT19EKaNn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aBD3fRknjEj6",
        "9TpGEYTblzQc",
        "wNJM0LG1iziA",
        "gLHjV3q28LNr",
        "pewE01Ca4BG0",
        "QXXrs_PjAHrN",
        "wxcJqnb1Him4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}